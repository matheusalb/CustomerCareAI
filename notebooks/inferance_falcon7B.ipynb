{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/matheusalb/anaconda3/envs/llm/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoConfig,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments)\n",
    "import transformers\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_prepare_model(model_name):\n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"query_key_value\"\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, peft_config, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text):\n",
    "    text_token = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    text_token = text_token.to('cuda:0')\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(\n",
    "            input_ids = text_token.input_ids, \n",
    "            max_new_tokens=200,\n",
    "            # temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.48s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'tiiuae/falcon-7b-instruct'\n",
    "model, peft_config, tokenizer = create_and_prepare_model(model_name)\n",
    "model = PeftModel.from_pretrained(model, './results/test_falcon_instruct_novoprompt/checkpoint-100')\n",
    "gen_input = lambda x: f'''\n",
    "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: {x}\\n\n",
    "###'''               "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Alimentação caríssima para um péssimo atendimento.. super mal atendido\n",
      "\n",
      "###\n",
      "Prezado cliente, lamentamos muito pelo o que você passou em nosso restaurante. Nós levamos a qualidade dos nossos alimentos muito a sério e ficamos extremamente desapontados em saber que não conseguimos atender às suas expectativas. \n",
      "\n",
      "Gostaríamos de pedir desculpas pelo comportamento inadequado dos nossos servidores. Ainda assim, estamos treinando nossa equipe para garantir que isso não aconteça novamente. Nos anos à venir, esperamos ter a oportunidade de servi-lo novamente em nosso restaurante e mostrar que podemos oferecer uma experiência de refeição excepcional. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos melhorar nossa prestação. \n",
      "\n",
      "Esperamos ter a oportunidade de servi-lo novamente em nosso restaurante e mostrar que podemos oferecer uma experiência de refeição excepcional. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos melhorar nossa prestação.\n",
      "\n",
      "###\n",
      "\n",
      "Este é um comentário de resposta ao seu comentário de cliente ao seu restaurante. \n",
      "Gostaríamos de pedir desculpas pelo comportamento inadequado dos nossos servidores. Ainda assim, estamos treinando nossa equipe para garantir que isso não aconteça novamente. Nos anos à venir, esperamos ter a oportunidade de servi-lo novamente em nosso restaurante e mostrar que podemos oferecer uma experiência de refeição excepcional. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos melhorar nossa prestação.\n",
      "\n",
      "###\n",
      "Prezado cliente, esperamos ter a oportunidade de servi-lo novamente em nosso restaurante e mostrar que podemos oferecer uma experiência de refeição excepcional. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos melhorar nossa prestação.\n",
      "\n",
      "###\n",
      "\n",
      "Este é um comentário de resposta ao seu comentário de cliente ao nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "\n",
      "###\n",
      "Prezado cliente, lamentamos muito pelo ocorrido em nosso restaurante. Nós levamos a qualidade dos nossos alimentos muito a sério e ficamos extremamente desapontados em saber que o seu experiência não foi satisfatória. Gostaríamos de pedir desculpas pelo atendimento inadequado e pelo tempo de espera prolongado.\n",
      "\n",
      "Gostaríamos de informar que já tomamos medidas para garantir que isso não aconteça novamente, incluindo treinamento adicional para a nossa equipe e revisão dos nossos processos de qualidade. \n",
      "Agradecemos pelo seu feedback e esperamos ter a oportunidade de servi-lo novamente em um futuro próximo, para que possamos mostrar a você a qualidade que sempre buscamos oferecer aos nossos clientes. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou para registrar um comentário favorável.\n",
      "\n",
      "Esperamos ter a oportunidade de servi-lo novamente em um futuro próximo e mostrar que podemos oferecer uma experiência de qualidade inigualável. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou para registrar um comentário favorável.\n",
      "\n",
      "Esperamos ter a oportunidade de servi-lo novamente em um futuro próximo e mostrar que podemos oferecer uma experiência de qualidade inigualável. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou para registrar um comentário favorável.\n",
      "\n",
      "Agradecemos pelo seu feedback e esperamos ter a oportunidade de servi-lo novamente em um futuro próximo, para que possamos mostrar a você a qualidade que sempre buscamos oferecer aos nossos clientes. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou para registrar um comentário favorável.\n",
      "\n",
      "Esperamos ter a oportunidade de servi-lo novamente em um futuro próximo e mostrar que podemos oferecer uma experiência de qualidade inigualável. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou para registrar um comentário favorável.\n",
      "\n",
      "Agradecemos pelo seu feedback e esperamos ter\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "\n",
      "###\n",
      "Prezado cliente, lamentamos muito pelo o tempo de espera da sua comida em nosso restaurante. Nós levamos a qualidade dos nossos alimentos muito a sério e ficamos extremamente desapontados em saber que a sua experiência não foi satisfatória. Pedimos desculpas pelo atraso na entrega da sua comida e pela falta de qualidade na preparação. \n",
      "Gostaríamos de informar que já tomamos medidas para garantir que isso não aconteça novamente, incluindo treinamento adicional para nossa equipe de cozinha e revisão dos nossos processos de preparação dos alimentos. \n",
      "Agradecemos pelo seu feedback e esperamos ter a oportunidade de servi-lo novamente em um futuro próximo, para que possamos mostrar a você a qualidade que sempre buscamos oferecer aos nossos clientes. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos ajudar você a uma qualidade de comida excepcional. \n",
      "Esperamos ter a oportunidade de servi-lo novamente em um futuro próximo e mostrar que a qualidade é a nossa prioridade. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos ajudar você a uma qualidade de comida excepcional. \n",
      "Esperamos ter a oportunidade de servi-lo novamente em um futuro próximo e mostrar que a qualidade é a nossa prioridade. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos ajudar você a uma qualidade de comida excepcional. \n",
      "Esperamos ter a oportunidade de servi-lo novamente em um futuro próximo e mostrar que a qualidade é a nossa prioridade. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos ajudar você a uma qualidade de comida excepcional. \n",
      "Agradecemos pelo seu feedback e esperamos ter a oportunidade de servi-lo novamente em um futuro próximo\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "\n",
      "###\n",
      "Prezado cliente, lamentamos muito pelo ocorrido em nosso restaurante. Sabemos que o tempo de espera na frente dos clientes é muito importante para a nossa empresa e para os nossos clientes.\n",
      "\n",
      "Gostaríamos de pedir desculpas pelo atraso na entrega dos seus pedidos e pela falta de atendimento na área interna. Nós levamos a qualidade dos nossos alimentos muito a sério e ficamos extremamente desapontados em saber que não conseguimos atender às nossas expectativas durante os fins de semana.\n",
      "\n",
      "Agradecemos pelo seu feedback e esperamos ter a oportunidade de servi-lo novamente em um futuro próximo, para que possamos mostrar a você a qualidade que sempre buscamos oferecer aos nossos clientes. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos melhorar nossa entrega e atendimento.\n",
      "\n",
      "Esperamos ter a oportunidade de servi-lo novamente em um futuro próximo e mostrar que podemos oferecer uma experiência de qualidade que você espera e merece. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos melhorar nossa entrega e atendimento.\n",
      "\n",
      "Esperamos ter a oportunidade de servi-lo novamente em um futuro próximo e mostrar que podemos oferecer uma experiência de qualidade que você espera e merece. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos melhorar nossa entrega e atendimento.\n",
      "\n",
      "Agradecemos pelo seu feedback e esperamos ter a oportunidade de servi-lo novamente em um futuro próximo, para que possamos mostrar a você a qualidade que sempre buscamos oferecer aos nossos clientes. Por favor, não hesite em entrar em contato conosco se precisar de mais informações ou se houver alguma forma que nós podemos melhorar nossa entrega e atendimento.\n",
      "\n",
      "Esperamos ter a oportunidade de servi-lo novamente em um futuro próximo e mostrar que podemos oferecer uma experiência de qualidade que você espera e merece. Por\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
