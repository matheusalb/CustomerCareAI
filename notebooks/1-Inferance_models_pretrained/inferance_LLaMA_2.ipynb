{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/matheusalb/anaconda3/envs/llm/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoConfig,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments)\n",
    "import transformers\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_prepare_model(model_name):\n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"query_key_value\"\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, peft_config, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa34bbda486b42d790b378158da0bcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0f5719309d48b9beb81350353bca6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4154b7f0a8cf4f37affd006b07b45508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549b651305cb47e7b025eb88fecd95c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5988da81c14b2487d5b930b9894614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "model, peft_config, tokenizer = create_and_prepare_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(model, './results/teste_1000iteracoes/checkpoint-1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input_1 = lambda x: f\"### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante. \\\n",
    "Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante. \\\n",
    "Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa, \\\n",
    "empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e \\\n",
    "que ele poderá voltar a fazer pedidos no restaurante. \\\n",
    "Certifique-se de usar detalhes específicos do comentário do cliente.\\n \\\n",
    "{x}\\n\\\n",
    "### Reply: \"\n",
    "\n",
    "gen_input_2 = lambda x: f\"Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: {x}\"\n",
    "\n",
    "gen_input_3 = lambda x: f\"Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: {x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text):\n",
    "    text_token = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    text_token = text_token.to('cuda:0')\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(\n",
    "            input_ids = text_token.input_ids, \n",
    "            max_new_tokens=200,\n",
    "            # temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante. Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante. Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa, empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e que ele poderá voltar a fazer pedidos no restaurante. Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      " Alimentação caríssima para um péssimo atendimento.. super mal atendido\n",
      "### Reply: \n",
      "Olá,\n",
      "\n",
      "Como você pode ver, o cliente está muito desapontado com o atendimento que recebeu.\n",
      "\n",
      "Eu vou fazer tudo o que puder para resolver este problema.\n",
      "\n",
      "Para começar, vou fazer uma revisão do que aconteceu.\n",
      "\n",
      "O cliente disse que ele estava muito desapontado com o atendimento que recebeu.\n",
      "\n",
      "Eu vou fazer tudo o que puder para resolver este problema.\n",
      "\n",
      "Para começar, vou fazer uma revisão do que aconteceu.\n",
      "\n",
      "O cliente disse que ele estava muito desapontado com o atendimento que recebeu.\n",
      "\n",
      "Eu vou fazer tudo o que puder para resolver este problema.\n",
      "\n",
      "Para começar, vou fazer uma revisão do que aconteceu.\n",
      "\n",
      "O cliente disse\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante. Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante. Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa, empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e que ele poderá voltar a fazer pedidos no restaurante. Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      " Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "### Reply: \n",
      "\n",
      "Obrigado pelo seu comentário. Nós estamos muito preocupados com a sua experiência. Nós estamos investigando o problema e estamos trabalhando para resolver isso. Nós estamos muito dispostos a fazer o possível para que você possa voltar a fazer pedidos no nosso restaurante.\n",
      "\n",
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante. Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante. Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa, empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e que ele poder\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante. Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante. Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa, empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e que ele poderá voltar a fazer pedidos no restaurante. Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      " Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "### Reply: \n",
      "\n",
      "Olá,\n",
      "\n",
      "Como você pode ver, o lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "\n",
      "Não queremos que você se sinta desconfortável, então, vamos fazer uma mudança imediata.\n",
      "\n",
      "Você pode voltar a fazer pedidos no nosso restaurante.\n",
      "\n",
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante. Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante. Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa, empática e não genérica, convencendo o cliente que medidas\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante. Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante. Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa, empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e que ele poderá voltar a fazer pedidos no restaurante. Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      " Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "### Reply: \n",
      "\n",
      "> Obrigado por me informar sobre o problema.\n",
      "\n",
      "> Nós estamos atualmente trabalhando para resolver esse problema.\n",
      "\n",
      "> Nós estamos atualmente trabalhando para resolver esse problema.\n",
      "\n",
      "> Nós estamos atualmente trabalhando para resolver esse problema.\n",
      "\n",
      "> Nós estamos atualmente trabalhando para resolver esse problema.\n",
      "\n",
      "> Nós estamos atualmente trabalhando para resolver esse problema.\n",
      "\n",
      "> Nós estamos atualmente trabalhando para resolver esse problema.\n",
      "\n",
      "> Nós estamos atualmente trabalhando para resolver esse problema.\n",
      "\n",
      "> Nós estamos atualmente trabalhando para resolver esse problema.\n",
      "\n",
      "> Nós estamos atualmente trabalhando para resolver esse problema.\n",
      "\n",
      "> Nós estamos atualmente trabalhando para resol\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Alimentação caríssima para um péssimo atendimento.. super mal atendido, não recomendo. Hinweis: Die meisten der oben genannten Spiele sind auch auf dem PC verfügbar.\n",
      "Their fans offended him, and (during his rookie season) so did his own teammates, some of whom got into trouble for misbehaving. The NFL is allowing liquor commercials this season. The 49ers dropped to 0 6 with a 26 24 loss to Washington.\n",
      "Their fans offended him, and (during his rookie season) so did his own teammates, some of whom got into trouble for misbehaving. The NFL is allowing liquor commercials this season. The 49ers dropped to 0 6 with a 26 24 loss to Washington.\n",
      "The 49ers dropped to 0 6 with a 26 24 loss to Washington. Their fans offended\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "Ao ler isso, o proprietário do restaurante ficou furioso e decidiu que o cliente não merecia mais um serviço. Ele enviou um e-mail para o cliente, que foi publicado no site do restaurante.\n",
      "O proprietário do restaurante, que não queria ser identificado, disse que o cliente foi muito grosseiro e que ele não merecia mais um serviço.\n",
      "\"Eu não quero que ele vem mais para o meu restaurante. Ele é um idiota. Ele não merece mais um serviço. Ele é um idiota. Ele não merece mais um serviço. Ele é um idiota. Ele não merece mais um serviço. Ele é um idiota. Ele não merece mais um serviço. Ele é um idiota. Ele não merece mais um serviço. Ele é um idiota. Ele não\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta. Hinweis: Die Bewertung wird angezeigt, nachdem alle Kunden ihre Bewertung abgegeben haben.\n",
      "The restaurant is located in the heart of the city, and is a popular place for locals and tourists alike.\n",
      "The restaurant is known for its delicious food, and the staff is always friendly and helpful.\n",
      "The restaurant is always busy, and the food is always delicious.\n",
      "The restaurant is always clean, and the staff is always friendly.\n",
      "The restaurant is always busy, and the food is always delicious.\n",
      "The restaurant is always clean, and the staff is always friendly.\n",
      "The restaurant is always busy, and the food is always delicious.\n",
      "The restaurant is always clean, and the staff is always friendly.\n",
      "The restaurant is always busy, and the food is always delicious.\n",
      "The restaurant is always clean, and the staff is always friendly.\n",
      "The restaurant is always busy, and the food is always delicious\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido. sierpni 2018.\n",
      "The 2018 FIFA World Cup was the 21st FIFA World Cup, an international football tournament contested by the men’s national teams of the member associations of FIFA once every four years. It took place in Russia from 14 June to 15 July 2018. It was the first World Cup to be held in Eastern Europe, and the 11th time that it had been held in Europe. At an estimated cost of over $14.2 billion, it.\n",
      "The 2018 FIFA World Cup was the 21st FIFA World Cup, an international football tournament contested by the men’s national teams of the member associations of FIFA once every four years. It took place in Russia from 14 June to 15 July 2018. It was the first World Cup to be held in Eastern Europe, and the 11th\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Alimentação caríssima para um péssimo atendimento.. super mal atendido, não recomendo. Hinweis: Die meisten der oben genannten Spiele sind auch auf dem PC verfügbar.\n",
      "Their fans offended him, and (during his rookie season) so did his own teammates, some of whom got into trouble for misbehaving. The NFL is allowing liquor commercials this season. The 49ers dropped to 0 6 with a 26 24 loss to Washington.\n",
      "Their fans offended him, and (during his rookie season) so did his own teammates, some of whom got into trouble for misbehaving. The NFL is allowing liquor commercials this season. The 49ers dropped to 0 6 with a 26 24 loss to Washington.\n",
      "The 49ers dropped to 0 6 with a 26 24 loss to Washington. Their fans offended\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um fr\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta. sierpni 2018.\n",
      "I’m sorry to hear that you had a bad experience at our restaurant. I’m glad you were able to get a refund.\n",
      "I’m sorry to hear that you had a bad experience at our restaurant. I’m glad you were able to get a refund. I’m sorry to hear that you had a bad experience at our restaurant. I’m glad you were able to get a refund.\n",
      "I’m sorry to hear that you had a bad experience at our restaurant. I’m glad you were able to get a refund. I’m sorry to hear that you had a bad experience at our restaurant. I’m glad you were able to get a refund. I’m sorry to hear that you had a bad experience at our restaurant. I’m glad you were able to get a refund. I’m sorry to hear that you had a bad experience at our restaurant\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido. sierpni 2018.\n",
      "The first thing you need to do is to find a good essay writing service. You can find a lot of them online, but you need to be careful. You need to find a service that is reliable and has a good reputation. You also need to find a service that is affordable.\n",
      "The second thing you need to do is to find a good essay writing service. You can find a lot of them online, but you need to be careful. You need to find a service that is reliable and has a good reputation. You also need to find a service that is affordable.\n",
      "The third thing you need to do is to find a good essay writing service. You can find a lot of them online, but you need to be careful. You need to find a service that is reliable and has a good reputation. You also need to find a service that is affordable.\n",
      "The fourth thing you need to do is\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
