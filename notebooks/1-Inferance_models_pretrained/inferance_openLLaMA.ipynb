{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/matheusalb/anaconda3/envs/llm/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoConfig,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments)\n",
    "import transformers\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_prepare_model(model_name):\n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"query_key_value\"\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, peft_config, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746e8a5223e94886a3885ea0fc5433fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'openlm-research/open_llama_7b_v2'\n",
    "model, peft_config, tokenizer = create_and_prepare_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(model, './results/teste_1000iteracoes/checkpoint-1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input_1 = lambda x: f\"### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante. \\\n",
    "                Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante. \\\n",
    "                Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa, \\\n",
    "                empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e \\\n",
    "                que ele poderá voltar a fazer pedidos no restaurante. \\\n",
    "                Certifique-se de usar detalhes específicos do comentário do cliente.\\n \\\n",
    "                <{x}>\\n\\\n",
    "                ### Reply: \"\n",
    "\n",
    "gen_input_2 = lambda x: f\"Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: {x}\"\n",
    "\n",
    "gen_input_3 = lambda x: f\"Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: {x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text):\n",
    "    text_token = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    text_token = text_token.to('cuda:0')\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(\n",
    "            input_ids = text_token.input_ids, \n",
    "            max_new_tokens=200,\n",
    "            # temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante.                 Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante.                 Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa,                 empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e                 que ele poderá voltar a fazer pedidos no restaurante.                 Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      "                 <Alimentação caríssima para um péssimo atendimento.. super mal atendido>\n",
      "                ### Reply: \"Obrigado por nos dar a oportunidade de melhorar nosso serviço. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante.                 Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante.                 Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa,                 empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e                 que ele poderá voltar a fazer pedidos no restaurante.                 Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      "                 <Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!>\n",
      "                ### Reply: <Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!>\n",
      "                ### Reply: <Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!>\n",
      "                ### Reply: <Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!>\n",
      "                ### Reply: <Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!>\n",
      "                ### Reply: <Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante.                 Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante.                 Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa,                 empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e                 que ele poderá voltar a fazer pedidos no restaurante.                 Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      "                 <Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.>\n",
      "                ### Reply: \"Obrigado por nos ter chamado para resolver seu problema. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante.                 Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante.                 Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa,                 empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e                 que ele poderá voltar a fazer pedidos no restaurante.                 Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      "                 <Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.>\n",
      "                ### Reply: \"Obrigado por nos ter chamado. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu problema o mais rápido possível. Nós vamos tentar resolver seu\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Alimentação caríssima para um péssimo atendimento.. super mal atendido.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "Agradecemos a sua opinião.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "\n",
      "A resposta deve ser escrita em forma de e-mail e deve ser enviada para o e-mail do cliente.\n",
      "\n",
      "A resposta deve ser escrita em forma de e-mail e deve ser enviada para o e-mail do cliente.\n",
      "\n",
      "A resposta deve ser escrita em forma de e-mail e deve ser enviada para o e-mail do cliente.\n",
      "\n",
      "A resposta deve ser escrita em forma de e-mail e deve ser enviada para o e-mail do cliente.\n",
      "\n",
      "A resposta deve ser escrita em forma de e-mail e deve ser enviada para o e-mail do cliente.\n",
      "\n",
      "A resposta deve ser escrita em forma de e-mail e deve ser enviada para o e-mail\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "O restaurante é um restaurante de hambúrguer, com 2 salas de jantar e 1 salão de espetáculos.\n",
      "O restaurante tem 2 salas de jantar e 1 salão de espetáculos.\n",
      "O restaurante tem 2 salas de jantar e 1 salão de espetáculos.\n",
      "O restaurante tem 2 salas de jantar e 1 salão de espetáculos.\n",
      "O restaurante tem 2 salas de jantar e 1 salão de espetáculos.\n",
      "O restaurante tem 2 salas de jantar e 1 salão de espetáculos.\n",
      "O restaurante tem 2 salas de jantar e 1 salão de espetáculos.\n",
      "O restaurante tem 2 salas de jantar e 1 salão de espet\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Alimentação caríssima para um péssimo atendimento.. super mal atendido.\n",
      "O cliente escreveu:\n",
      "I was in your restaurant last night with my family. We were very disappointed with the service. We were seated at a table that was not cleaned. We had to ask for a clean table. We were not offered a menu. We had to ask for a menu. We were not offered a drink menu. We had to ask for a drink menu. We were not offered a bread basket. We had to ask for a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket. We were not offered a bread basket\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n",
      "O cliente não gostou do frango.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "O cliente escreveu:\n",
      "“I was there last night with my wife and we had a great time. The food was excellent and the service was great. The only problem was that the place was very hot and the food took a long time to get to the table. I would recommend this place to anyone who wants to have a great time and eat some great food.”\n",
      "O cliente escreveu:\n",
      "“I was there last night with my wife and we had a great time. The food was excellent and the service was great. The only problem was that the place was very hot and the food took a long time to get to the table. I would recommend this place to anyone who wants to have a great time and eat some great food.”\n",
      "O cliente escreveu:\n",
      "“I was there last night with my wife and we had a great time. The food was excellent and the service was great. The only problem was that the place was very hot and the\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "O restaurante é um restaurante de hambúrguer, com 2 salas de jantar, uma para os clientes internos e outra para os clientes externos.\n",
      "O restaurante tem 2 funcionários de atendimento, um para cada sala.\n",
      "O funcionário interno tem 2 salas de jantar para atender, uma com 10 mesas e a outra com 8 mesas.\n",
      "O funcionário externo tem 1 sala de jantar para atender, com 8 mesas.\n",
      "O funcionário interno tem 1 funcionário externo para ajudar, com 1 funcionário interno para ajudar.\n",
      "O funcionário externo tem 1 funcionário interno para ajudar, com 1 funcionário interno para ajudar.\n",
      "O funcionário interno tem 1 funcionário externo para ajudar, com 1 funcionário interno para ajudar\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
