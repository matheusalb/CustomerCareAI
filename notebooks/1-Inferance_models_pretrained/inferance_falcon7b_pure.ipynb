{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/matheusalb/anaconda3/envs/llm/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoConfig,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments)\n",
    "import transformers\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_prepare_model(model_name):\n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"query_key_value\"\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, peft_config, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.42s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'tiiuae/falcon-7b-instruct'\n",
    "model, peft_config, tokenizer = create_and_prepare_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(model, './results/teste_1000iteracoes/checkpoint-1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input_1 = lambda x: f\"### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante. \\\n",
    "                Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante. \\\n",
    "                Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa, \\\n",
    "                empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e \\\n",
    "                que ele poderá voltar a fazer pedidos no restaurante. \\\n",
    "                Certifique-se de usar detalhes específicos do comentário do cliente.\\n \\\n",
    "                <{x}>\\n\\\n",
    "                ### Reply: \"\n",
    "\n",
    "gen_input_2 = lambda x: f\"Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: {x}\"\n",
    "\n",
    "gen_input_3 = lambda x: f\"Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: {x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text):\n",
    "    text_token = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    text_token = text_token.to('cuda:0')\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(\n",
    "            input_ids = text_token.input_ids, \n",
    "            max_new_tokens=200,\n",
    "            # temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante.                 Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante.                 Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa,                 empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e                 que ele poderá voltar a fazer pedidos no restaurante.                 Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      "                 <Alimentação caríssima para um péssimo atendimento.. super mal atendido>\n",
      "                ### Reply: \n",
      "O cliente foi mal atendido no restaurante. A sua atitude foi totalmente inaccecível e não foi atendido com a atenção que se esperava. A sua falta de empatia foi notada e foi totalmente inaceitável. A sua falta de empatia foi notada e foi totalmente inaceitável. A sua falta de empatia foi notada e foi totalmente inaceitável. A sua falta de empatia foi notada e foi totalmente inaceitável. A sua falta de empatia foi notada e foi totalmente inaceitável. A sua falta de empatia foi notada e foi totalmente inaceitável. A sua falta de empatia foi notada e foi totalmente inaceitável. A sua falta de empatia foi notada e foi totalmente inaceitável. A sua falta de empatia foi notada e foi totalmente inaceitável. A sua\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante.                 Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante.                 Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa,                 empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e                 que ele poderá voltar a fazer pedidos no restaurante.                 Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      "                 <Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!>\n",
      "                ### Reply: <p>Comuniquei com o cliente que o frango foi preparado com a melhor qualidade e foi feito com a maior precisão. Agradeci-o por ter nos comentou e prometo que o frango foi preparado com a melhor qualidade e foi feito com a maior precisão. Agradeci-o por ter nos comentou e prometo que o frango foi preparado com a melhor qualidade e foi feito com a maior precisão. </p>\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante.                 Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante.                 Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa,                 empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e                 que ele poderá voltar a fazer pedidos no restaurante.                 Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      "                 <Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.>\n",
      "                ### Reply: \n",
      "O cliente foi atendido com a maior rapidez possível, mas ainda foi atrasado pela falta de ação do restaurante. A comida foi preparada com a maior rapidez possível, mas ainda foi atrasada pela falta de ação do restaurante. A falta de ação do restaurante foi o único problema do cliente, mas foi atencionado e foi atendido com a maior rapidez possível. A comida foi preparada com a maior rapidez possível, mas ainda foi atrasada pela falta de ação do restaurante. A falta de ação do restaurante foi o único problema do cliente, mas foi atencionado e foi atendido com a maior rapidez possível.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Você é um especialista em responder comentários negativos de um cliente a um restaurante.                 Sua tarefa é responder respeitosamente um comentário negativo de um cliente ao seu restaurante.                 Dado o comentário do cliente entre, escreva em Português um comentário de resposta de forma respeitosa,                 empática e não genérica, convencendo o cliente que medidas serão tomadas para resolver o seu problema e                 que ele poderá voltar a fazer pedidos no restaurante.                 Certifique-se de usar detalhes específicos do comentário do cliente.\n",
      "                 <Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.>\n",
      "                ### Reply: \n",
      "O cliente foi atendido por um especialista em responder comentários negativos de um cliente a um restaurante. O especialista foi educado e empático, convencendo o cliente que medidas serão tomadas para resolver o seu problema e que ele poderá voltar a fazer pedidos no restaurante. O cliente foi atendido com detalhes específicos do comentário do cliente, mas foi também acolhido com o atendimento externo, pois quem fica do lado de fora espera cerca de 40 min para ser atendido.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input_1(comment))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Alimentação caríssima para um péssimo atendimento.. super mal atendido.\n",
      "Alimentação caríssima para um péssimo atendimento. super mal atendido.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "I'm sorry to hear that you had a bad experience at our restaurant. We strive to provide the best service and food to our customers, and it's disheartening to hear that we fell short in your case. We will be reviewing our processes to ensure that we can improve in the future. Thank you for taking the time to share your feedback.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "I'm sorry to hear that you had a slow service at your restaurant. It's important to provide timely service to your customers. I hope that you have addressed this issue with your staff and have taken steps to improve your service in the future.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva em Português um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "I'm sorry to hear that you had a bad experience at our restaurant. We strive to provide the best service and food to our customers, and it's disheartening to hear that we fell short in your case. I would like to take this opportunity to apologize for the inconvenience you faced and to let you know that we are taking steps to ensure that this does not happen in the future. We value your feedback and appreciate your patience. Thank you for taking the time to share your thoughts with us.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input_2(comment))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Alimentação caríssima para um péssimo atendimento.. super mal atendido.\n",
      "I'm sorry to hear that you had a bad experience at our restaurant. We strive to provide excellent service and food to our customers, and it's disheartening to hear that we fell short in your case. I will make sure to address the issue with our staff and management to ensure that it doesn't happen again. Thank you for taking the time to share your feedback, and we hope that we can serve you better in the future.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "I'm sorry to hear that you had a bad experience at our restaurant. We strive to provide the best service and food to our customers, and it's disheartening to hear that we fell short in your case. We will be reviewing our processes to ensure that we can improve in the future. Thank you for taking the time to share your feedback, and we hope that we can serve you again in the future.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "O cliente foi atendido por um empregado que não era muito amável.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min para ser atendido.\n",
      "Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min para ser atendido.\n",
      "Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min para ser atendido.\n",
      "Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min para ser atendido.\n",
      "Com o aumento do público nos finais de semana,\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input_3(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
