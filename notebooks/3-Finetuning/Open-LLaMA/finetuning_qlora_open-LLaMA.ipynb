{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/matheusalb/anaconda3/envs/llm/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import bitsandbytes as bnb\n",
    "import datetime\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset \n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4_bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "    peft_config =  LoraConfig(\n",
    "        lora_alpha=16, # 16\n",
    "        lora_dropout=0.1, # 0.05\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=64, # 8\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"v_proj\",\n",
    "            # \"k_proj\",\n",
    "            # \"o_proj\"\n",
    "        ]\n",
    "    )\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        # bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model, True)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        model_name\n",
    "    )\n",
    "    tokenizer.pad_token = (\n",
    "        0\n",
    "    )\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def create_and_prepare_model(model_name):\n",
    "    return get_model(model_name), get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset(path):\n",
    "    instruction = 'Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante'\n",
    "    messages = {}\n",
    "    messages['instruction'] =[]\n",
    "    messages['input'] = []\n",
    "    messages['output'] = [] \n",
    "    \n",
    "    df = pd.read_csv(path, skip_blank_lines=True)\n",
    "    df = df.dropna(how='all')\n",
    "    for _, linha in df.iterrows():\n",
    "        messages['instruction'].append(instruction)\n",
    "        messages['input'].append(linha['comentario'])\n",
    "        messages['output'].append(linha['sugestaoResposta'])\n",
    "            \n",
    "    return Dataset.from_dict(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    prompt = f'''\\\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{data_point['instruction']}\n",
    "### Input:\n",
    "{data_point['input']}\n",
    "### Response:\n",
    "{data_point['output']}\n",
    "'''.strip()               \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, max_lenght=300, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_lenght,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    ## adicionando eos token ao final dos tokens\n",
    "    if (\n",
    "        result['input_ids'][-1] != tokenizer.eos_token_id\n",
    "        and len(result['input_ids']) < max_lenght\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result['input_ids'].append(tokenizer.eos_token_id)\n",
    "        result['attention_mask'].append(1)\n",
    "        \n",
    "    # generative models: labels are the same as the input\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Tokenize the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_prompt = tokenize(full_prompt)\n",
    "    return tokenized_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'openlm-research/open_llama_7b_v2'\n",
    "train_path = '../../../data/train_base.csv'\n",
    "validation_path = '../../../data/validation_base.csv'    \n",
    "OUTPUT_DIR = \"./results/qlora_falcon7b_transformer_trainer\"+datetime.datetime.now().isoformat()\n",
    "\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2, # micro_batch_size\n",
    "    gradient_accumulation_steps=4, # batch_size / micro_batch_size\n",
    "    # num_train_epochs=80,\n",
    "    max_steps=100,\n",
    "    save_steps=100,\n",
    "    optim=\"paged_adamw_32bit\", # adamw_torch\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4, # 3e-4\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03, #0.05\n",
    "    # warmup_steps=100,\n",
    "    lr_scheduler_type=\"constant\", #cosine\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=20,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.24s/it]\n",
      "Downloading tokenizer.model: 100%|██████████| 512k/512k [00:00<00:00, 1.10MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 330/330 [00:00<00:00, 3.08MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 593/593 [00:00<00:00, 5.72MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,554,432 || all params: 3,533,967,360 || trainable%: 0.9494833591219133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = create_and_prepare_model(model_name)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "train_data = get_dataset(train_path)\n",
    "validation_data = get_dataset(validation_path)\n",
    "\n",
    "train_data = train_data.map(generate_and_tokenize_prompt)\n",
    "validation_data = validation_data.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_to_multiple_of -> An integer representing the maximum sequence length, rounded up to the nearest multiple of this value.\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatheusalb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matheusalb/Documents/CustomerCareAI/notebooks/3-Finetuning/Open-LLaMA/wandb/run-20230719_020411-30oait9b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matheusalb/huggingface/runs/30oait9b' target=\"_blank\">cerulean-haze-31</a></strong> to <a href='https://wandb.ai/matheusalb/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matheusalb/huggingface' target=\"_blank\">https://wandb.ai/matheusalb/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matheusalb/huggingface/runs/30oait9b' target=\"_blank\">https://wandb.ai/matheusalb/huggingface/runs/30oait9b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:46<06:37,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8737, 'learning_rate': 0.0002, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [01:33<05:59,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3314, 'learning_rate': 0.0002, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|██        | 20/100 [01:52<05:59,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1879265308380127, 'eval_runtime': 18.8869, 'eval_samples_per_second': 7.201, 'eval_steps_per_second': 0.9, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [02:38<05:37,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0074, 'learning_rate': 0.0002, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [03:25<04:36,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9674, 'learning_rate': 0.0002, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|████      | 40/100 [03:44<04:36,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9868166446685791, 'eval_runtime': 19.1702, 'eval_samples_per_second': 7.094, 'eval_steps_per_second': 0.887, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [04:31<04:02,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8532, 'learning_rate': 0.0002, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [05:18<03:08,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8299, 'learning_rate': 0.0002, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 60%|██████    | 60/100 [05:37<03:08,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9497116208076477, 'eval_runtime': 18.958, 'eval_samples_per_second': 7.174, 'eval_steps_per_second': 0.897, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [06:17<02:05,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8282, 'learning_rate': 0.0002, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [06:58<01:20,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8727, 'learning_rate': 0.0002, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 80%|████████  | 80/100 [07:14<01:20,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9269622564315796, 'eval_runtime': 16.1606, 'eval_samples_per_second': 8.416, 'eval_steps_per_second': 1.052, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [07:53<00:41,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7507, 'learning_rate': 0.0002, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:34<00:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8333, 'learning_rate': 0.0002, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 100/100 [08:50<00:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9136545062065125, 'eval_runtime': 16.1668, 'eval_samples_per_second': 8.412, 'eval_steps_per_second': 1.052, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:00<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 545.5375, 'train_samples_per_second': 1.466, 'train_steps_per_second': 0.183, 'train_loss': 1.0148065090179443, 'epoch': 0.74}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_input(comment):\n",
    "    prompt = f'''Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
    "### Input:\n",
    "{comment}\n",
    "### Response:\n",
    "'''           \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config=model.generation_config\n",
    "generation_config.max_new_tokens=300\n",
    "# generation_config.temeperature=0.7\n",
    "# generation_config.top_k=0.9\n",
    "generation_config.num_return_sequences=1\n",
    "# generation_config.pad_token_id=tokenizer.eos_token_id\n",
    "# generation_config.eos_token_id=tokenizer.eos_token_id\n",
    "\n",
    "def inference(model, text):\n",
    "    enconded = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        input_ids=enconded.input_ids,\n",
    "        attention_mask=enconded.attention_mask,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1448: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Alimentação caríssima para um péssimo atendimento.. super mal atendido\n",
      "### Response:\n",
      "Olá, agradecemos por compartilhar sua experiência conosco. Lamentamos muito pela má impressão causada pelo atendimento insatisfatório que você recebeu. Vamos analisar o ocorrido para melhorar nossos serviços. Agradecemos por nos informar.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Filé estava duro. Ficamos decepcionados.\n",
      "### Response:\n",
      "Olá, agradecemos por compartilhar sua experiência conosco. Lamentamos que a comida não tenha atendido às suas expectativas. Vamos analisar o filé para garantir que isso não aconteça novamente. Agradecemos por nos informar.\n"
     ]
    }
   ],
   "source": [
    "#treino\n",
    "comment='Filé estava duro. Ficamos decepcionados.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "A falta de vaga para cadeirantes e idosos é uma falha terrível nos dias de hoje, além de ser lei não tivemos nenhum apoio ou mesmo interesse por parte dos funcionários para ajudar na questão do cadeirante que estava nos acompanhado.\n",
      "Não vou deixar de avaliar os outros pontos, no que tange ao atendimento interno e as delícias que são servidas. tudo de bom. \n",
      "Mas não vá com cadeirante que passará sufoco.\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback sobre a falta de vaga para cadeirantes e idosos em nosso restaurante. Lamentamos muito pela experiência negativa que você teve. Vamos analisar a situação para melhorar essa questão. Quanto ao atendimento interno, estamos sempre buscando melhorar nosso serviço para oferecer uma experiência agradável aos nossos clientes. Agradecemos por compartilhar sua opinião.\n"
     ]
    }
   ],
   "source": [
    "comment='''A falta de vaga para cadeirantes e idosos é uma falha terrível nos dias de hoje, além de ser lei não tivemos nenhum apoio ou mesmo interesse por parte dos funcionários para ajudar na questão do cadeirante que estava nos acompanhado.\n",
    "Não vou deixar de avaliar os outros pontos, no que tange ao atendimento interno e as delícias que são servidas. tudo de bom. \n",
    "Mas não vá com cadeirante que passará sufoco.'''\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "### Response:\n",
      "Olá, agradecemos por compartilhar sua experiência conosco. Lamentamos muito pela má impressão causada pelo atendimento demorado e pelo frango com gosto de peixe. Vamos analisar esses pontos para melhorar nossos serviços. Agradecemos por nos informar sobre esses detalhes. Esperamos ter a oportunidade de recebê-la novamente para oferecer uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "### Response:\n",
      "Olá, agradecemos por compartilhar sua experiência conosco. Lamentamos muito pela má impressão causada pelo atendimento demorado e pelo frango com gosto de peixe. Vamos analisar esses pontos para melhorar nossos serviços. Agradecemos por nos informar sobre esses detalhes. Esperamos ter a oportunidade de recebê-la novamente para oferecer uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "### Response:\n",
      "Olá, agradecemos por compartilhar sua experiência conosco. Lamentamos que a sua visita tenha sido difícil devido ao calor e à demora na preparação da comida. Vamos analisar esses pontos para melhorar nossos serviços. Esperamos ter a oportunidade de recebê-la novamente para proporcionar uma experiência mais satisfatória.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback. Lamentamos que tenha tido uma experiência negativa em nosso restaurante. Vamos analisar o seu comentário para melhorar nossos serviços. Agradecemos por nos informar sobre a dificuldade de atendimento na área interna com a área externa. Vamos trabalhar para resolver esse problema. Esperamos ter a oportunidade de recebê-la novamente para uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "O pior yakisoba que eu já comi na vida! Vem numa caixa com a massa toda amassada e o molho é horrível! Tive que jogar fora! Parecia comida de cachorro! Nunca mais!\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback sobre a nossa yakisoba. Lamentamos muito pela má experiência que você teve com a nossa comida. Vamos analisar o ocorrido para melhorar nossos processos de preparo. Agradecemos por nos informar sobre o problema. Esperamos ter a oportunidade de recebê-la novamente para oferecer uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = 'O pior yakisoba que eu já comi na vida! Vem numa caixa com a massa toda amassada e o molho é horrível! Tive que jogar fora! Parecia comida de cachorro! Nunca mais!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Cheguei para tomar café as 7h da manhã, pouquíssimas pessoas ,pedi um omelete levou 23 minutos a essa altura meu café já havia acabado, e chegou uma pessoa depois de mim e o pedido dela que era muito mais que o meu simples omelete saiu primeiro.\n",
      "Péssimo atendimento .\n",
      "Já fui bem atendida por outros  funcionários  mais essa turma da manhã 3 pessoas nota 2.\n",
      "### Response:\n",
      "Olá, agradecemos por compartilhar sua experiência conosco. Lamentamos muito pela má atendimento que você recebeu. Vamos analisar o ocorrido para evitar que isso aconteça novamente. Agradecemos por nos informar sobre o omelete que levou muito tempo para ser preparado.\n"
     ]
    }
   ],
   "source": [
    "comment= '''Cheguei para tomar café as 7h da manhã, pouquíssimas pessoas ,pedi um omelete levou 23 minutos a essa altura meu café já havia acabado, e chegou uma pessoa depois de mim e o pedido dela que era muito mais que o meu simples omelete saiu primeiro.\n",
    "Péssimo atendimento .\n",
    "Já fui bem atendida por outros  funcionários  mais essa turma da manhã 3 pessoas nota 2.'''\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Não recomendo! Não faz muito meu estilo cantar para quem não conheço, coisa que é impossível sem marcar antes. Sim, existem vários ambientes tocando estilos diferentes mas é necessário muita sorte para conseguir um lugar onde todos gostem do que está tocando. Sem falar que, se tratando do karaokê: ninguém é profissional por isso as performances são horríveis e a musica e letra são retiradas do YouTube, mas mesmo assim o espaço insiste em cobrar entrada! Para finalizar a comida, que é bem cara, veio fria e não gostei de nada que comi! Nem a bebida salvou minha noite, paguei um absurdo pela menor porção de caipirinha que já vi! Teria feito melhor na minha casa!\n",
      "### Response:\n",
      "Olá, agradeço por compartilhar sua experiência conosco. Lamentamos que você tenha tido uma má impressão com a nossa atmosfera e comida. Vamos analisar o que aconteceu para evitar que isso aconteça novamente. Quanto à entrada, estamos trabalhando para melhorar a experiência de todos os nossos clientes. Quanto à comida, vamos revisar nossos processos para garantir que a comida chegue aos nossos clientes em condições de qualidade. Quanto à karaoke, vamos revisar nossos processos para garantir que todos possam cantar em um ambiente agradável. Quanto à caipirinha, vamos revisar nossos processos para garantir que a quantidade de caipirinha que você recebe seja o que você esperava. Agradecemos por compartilhar sua experiência conosco e esperamos ter a oportunidade de recebê-la novamente para oferecer uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "#treino\n",
    "comment='Não recomendo! Não faz muito meu estilo cantar para quem não conheço, coisa que é impossível sem marcar antes. Sim, existem vários ambientes tocando estilos diferentes mas é necessário muita sorte para conseguir um lugar onde todos gostem do que está tocando. Sem falar que, se tratando do karaokê: ninguém é profissional por isso as performances são horríveis e a musica e letra são retiradas do YouTube, mas mesmo assim o espaço insiste em cobrar entrada! Para finalizar a comida, que é bem cara, veio fria e não gostei de nada que comi! Nem a bebida salvou minha noite, paguei um absurdo pela menor porção de caipirinha que já vi! Teria feito melhor na minha casa!'\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Eu gosto muito de tomar café aos domingos nessa panificadora, porém não sei o que esta acontecendo, ela está ficando muito suja.\n",
      "Gosto da parte de cima,mas quando chego as mesas estão todas sujas, com xícaras, resto de lanches e etc.\n",
      "Acredito que não estão higienizando.\n",
      "Outro detalhe a tapioca está vindo muito salgada.\n",
      "Espero que melhore, para que possamos continuar frequentando.\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback sobre a sua experiência em nosso restaurante. Lamentamos muito pela falta de limpeza e pela salinidade da tapioca. Vamos analisar esses pontos para melhorar nossos serviços. Agradecemos por nos informar sobre esses detalhes e esperamos ter a oportunidade de recebê-la novamente para uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "#treino\n",
    "comment = '''Eu gosto muito de tomar café aos domingos nessa panificadora, porém não sei o que esta acontecendo, ela está ficando muito suja.\n",
    "Gosto da parte de cima,mas quando chego as mesas estão todas sujas, com xícaras, resto de lanches e etc.\n",
    "Acredito que não estão higienizando.\n",
    "Outro detalhe a tapioca está vindo muito salgada.\n",
    "Espero que melhore, para que possamos continuar frequentando.'''\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Eu gostaria de deixar minha indignação quanto o atendimento de um garçom.\n",
      "Não estou  reclamando do restaurante.\n",
      "Estive no restaurante umas duas vezes, foi tudo muito bom.\n",
      "Mas me decepcionei na minha ultima vez.\n",
      "Foi atendida por um garçom mal educado e sem ética pois o mesmo alem de ser Grosso ficava olhando para as minhas pernas. Fiquei muito constrangida com a situação.\n",
      "Almoçamos tomamos. Sucos e cervejas, mesmo porque estavamos em 6 pessoas.\n",
      "No final pedimos a conta e o mesmo  trouxe duas contas uma com mais itens incluindo a refeiçao, outra somente com as bebidas,achei estranho porque tinham me cobrado normal das últimas vezes. Em fim.\n",
      "Como um gestor de um restaurante contrata pessoas sem conhecer seu carater?\n",
      "Tudo bem que a mão de obra de estrangeiros seja mais barata. Mas vocês teriam que pensar no bem estar dos clientes.\n",
      "Descobri o nome do garçom.\n",
      "CARLOS AMED. Ele é cubano.\n",
      "Ia fazer uma denuncia por assedio mas pensei na empresa. Por favor selecione melhor seus colaboradores.\n",
      "Muito indignada.\n",
      "### Response:\n",
      "Olá, agradecemos por compartilhar sua experiência conosco. Lamentamos muito pela má atuação do garçom que teve ocorrido. Vamos analisar o ocorrido para evitar que isso aconteça novamente. Agradecemos por nos informar sobre o nome do garçom. Vamos investigar e verificar se ele está em nossa lista de colaboradores. Agradecemos por nos informar e esperamos ter a oportunidade de recebê-la novamente para uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = '''Eu gostaria de deixar minha indignação quanto o atendimento de um garçom.\n",
    "Não estou  reclamando do restaurante.\n",
    "Estive no restaurante umas duas vezes, foi tudo muito bom.\n",
    "Mas me decepcionei na minha ultima vez.\n",
    "Foi atendida por um garçom mal educado e sem ética pois o mesmo alem de ser Grosso ficava olhando para as minhas pernas. Fiquei muito constrangida com a situação.\n",
    "Almoçamos tomamos. Sucos e cervejas, mesmo porque estavamos em 6 pessoas.\n",
    "No final pedimos a conta e o mesmo  trouxe duas contas uma com mais itens incluindo a refeiçao, outra somente com as bebidas,achei estranho porque tinham me cobrado normal das últimas vezes. Em fim.\n",
    "Como um gestor de um restaurante contrata pessoas sem conhecer seu carater?\n",
    "Tudo bem que a mão de obra de estrangeiros seja mais barata. Mas vocês teriam que pensar no bem estar dos clientes.\n",
    "Descobri o nome do garçom.\n",
    "CARLOS AMED. Ele é cubano.\n",
    "Ia fazer uma denuncia por assedio mas pensei na empresa. Por favor selecione melhor seus colaboradores.\n",
    "Muito indignada.'''\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Fui no dia 10/12/21 às 11:00 o restaurante não tinha ninguém ainda, fui fazer um pedido para viagem. Depois de quase 30 minutos de espera percebo que tinham clientes que chegaram depois comendo...questionei um garçom pela demora do pedido foi averiguar e não deu nenhuma satisfação. Aí percebi depois que o pedido estava no balcão aguardando o favor de algum garçom pegar e olha que nem estava lotado o restaurante...desrespeito com o cliente e garçons mal treinados.. nunca mais nesse lugar.\n",
      "### Response:\n",
      "Olá, agradeço por compartilhar sua experiência conosco. Lamentamos muito pela demora no atendimento e pelo desrespeito com o seu pedido. Vamos analisar o ocorrido para garantir que isso não aconteça novamente. Agradecemos por nos informar sobre o ocorrido.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Fui no dia 10/12/21 às 11:00 o restaurante não tinha ninguém ainda, fui fazer um pedido para viagem. Depois de quase 30 minutos de espera percebo que tinham clientes que chegaram depois comendo...questionei um garçom pela demora do pedido foi averiguar e não deu nenhuma satisfação. Aí percebi depois que o pedido estava no balcão aguardando o favor de algum garçom pegar e olha que nem estava lotado o restaurante...desrespeito com o cliente e garçons mal treinados.. nunca mais nesse lugar.\"\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Só não dou zero porque não dá e porque a torta de frango estava boazinha. Priorizam quem tem aparência de ter mais dinheiro e os garçons não fazem questão alguma de atender as pessoas. Olharam na minha cara, passaram direto até que eu implorasse pra ser atendida. :) Não anotaram meu pedido e ainda queriam colocar a culpa em mim pelo esquecimento. Enfim, não volto!\n",
      "### Response:\n",
      "Olá, agradecemos por compartilhar sua experiência conosco. Lamentamos que a sua visita não tenha sido satisfatória. Pedimos desculpas pelo ocorrido com a torta de frango e pelo atendimento insatisfatório. Vamos analisar o ocorrido para melhorar nossos serviços. Agradecemos por nos informar sobre esses pontos negativos e esperamos ter a oportunidade de recebê-la novamente para oferecer uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Só não dou zero porque não dá e porque a torta de frango estava boazinha. Priorizam quem tem aparência de ter mais dinheiro e os garçons não fazem questão alguma de atender as pessoas. Olharam na minha cara, passaram direto até que eu implorasse pra ser atendida. :) Não anotaram meu pedido e ainda queriam colocar a culpa em mim pelo esquecimento. Enfim, não volto!\"\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Pedimos um prato de bife ao molho madeira sendo que o feijão veio com gosto de molho madeira e o bife veio com gosto de queijo então eu recomendaria que vocês se dedicassem mais e as atende vistas são muito de humor baixo\n",
      "### Response:\n",
      "Olá, agradecemos por compartilhar sua experiência conosco. Lamentamos que você tenha tido uma má impressão com o prato de bife ao molho madeira. Vamos analisar o ocorrido para evitar que isso aconteça novamente. Quanto ao feijão, vamos revisar nossos processos para garantir que o gosto seja adequado. Agradecemos por nos informar sobre esses detalhes. Esperamos ter a oportunidade de recebê-la novamente para oferecer uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Pedimos um prato de bife ao molho madeira sendo que o feijão veio com gosto de molho madeira e o bife veio com gosto de queijo então eu recomendaria que vocês se dedicassem mais e as atende vistas são muito de humor baixo\"\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Pedi no ifood camarão empanado e mandaram camarão alho e óleo muito sem graça e poucas unidades. Fui inventar de experimentar por causa da propaganda no Instagram e me ferrei! Não recomendo.\n",
      "### Response:\n",
      "Olá, agradecemos por compartilhar sua experiência conosco. Lamentamos muito pela má impressão causada pelo serviço que você recebeu. Vamos analisar o ocorrido para melhorar nossos processos. Agradecemos por nos informar sobre a falta de graça e quantidade em nosso camarão empanado. Esperamos ter a oportunidade de recebê-la novamente para oferecer uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Pedi no ifood camarão empanado e mandaram camarão alho e óleo muito sem graça e poucas unidades. Fui inventar de experimentar por causa da propaganda no Instagram e me ferrei! Não recomendo.\"\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
