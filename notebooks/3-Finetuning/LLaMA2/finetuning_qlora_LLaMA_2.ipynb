{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/matheusalb/anaconda3/envs/llm/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from utils.create_and_prepare_model_qlora import (\n",
    "    create_and_prepare_LLaMA_model\n",
    ")\n",
    "\n",
    "import torch\n",
    "# import bitsandbytes as bnb\n",
    "import datetime\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset \n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    instruction = 'Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante'\n",
    "    messages = {}\n",
    "    messages['instruction'] =[]\n",
    "    messages['input'] = []\n",
    "    messages['output'] = [] \n",
    "    \n",
    "    df = pd.read_csv(path, skip_blank_lines=True)\n",
    "    df = df.dropna(how='all')\n",
    "    for _, linha in df.iterrows():\n",
    "        messages['instruction'].append(instruction)\n",
    "        messages['input'].append(linha['comentario'])\n",
    "        messages['output'].append(linha['sugestaoResposta'])\n",
    "            \n",
    "    return Dataset.from_dict(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    prompt = f'''\\\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{data_point['instruction']}\n",
    "### Input:\n",
    "{data_point['input']}\n",
    "### Response:\n",
    "{data_point['output']}\n",
    "'''.strip()               \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    ## adicionando eos token ao final dos tokens\n",
    "    if (\n",
    "        result['input_ids'][-1] != tokenizer.eos_token_id\n",
    "        and len(result['input_ids']) < MAX_LENGTH\n",
    "        and ADD_EOS_TOKEN\n",
    "    ):\n",
    "        result['input_ids'].append(tokenizer.eos_token_id)\n",
    "        result['attention_mask'].append(1)\n",
    "        \n",
    "    # generative models: labels are the same as the input\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Tokenize the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_prompt = tokenize(full_prompt)\n",
    "    return tokenized_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "train_path = '../../../data/base_2k/train_base.csv'\n",
    "validation_path = '../../../data/base_2k/validation_base.csv'    \n",
    "OUTPUT_DIR = \"./results/qlora_LLaMA_2_7B_transformer_trainer\"+datetime.datetime.now().isoformat()\n",
    "\n",
    "MAX_LENGTH=350\n",
    "ADD_EOS_TOKEN=True\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,\n",
    "#     bnb_8bit_quant_type=\"nf4\",\n",
    "#     bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_8bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "peft_config =  LoraConfig(\n",
    "    lora_alpha=16, # 16\n",
    "    lora_dropout=0.1, # 0.05\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=64, # 8\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        # \"k_proj\",\n",
    "        # \"o_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=4, # micro_batch_size\n",
    "    gradient_accumulation_steps=4, # batch_size / micro_batch_size, # per_device_train_batch_size * gradient_accumulation_steps = 16\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    # max_steps=100,\n",
    "    # save_steps=100,\n",
    "    optim=\"paged_adamw_32bit\", # adamw_torch\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4, # 3e-4\n",
    "    # fp16=True,\n",
    "    bf16=True, # testar\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03, #0.05\n",
    "    # warmup_steps=100,\n",
    "    lr_scheduler_type=\"constant\", #cosine\n",
    "    # evaluation_strategy='steps',\n",
    "    eval_steps=20,\n",
    "    # load_best_model_at_end=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    report_to=\"wandb\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013ad3eb199a4252bf3a0802cbe9d468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,554,432 || all params: 3,533,967,360 || trainable%: 0.9494833591219133\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = create_and_prepare_LLaMA_model(model_name, peft_config, bnb_config)\n",
    "\n",
    "tokenizer.pad_token = (\n",
    "    0\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.use_cache = False\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_to_multiple_of -> An integer representing the maximum sequence length, rounded up to the nearest multiple of this value.\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6328d959ad844560a282482a59d526bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ce49a3a6ff443da1de852908275f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = get_dataset(train_path)\n",
    "validation_data = get_dataset(validation_path)\n",
    "\n",
    "train_data = train_data.map(generate_and_tokenize_prompt)\n",
    "validation_data = validation_data.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatheusalb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matheusalb/Documents/CustomerCareAI/notebooks/3-Finetuning/LLaMA2/wandb/run-20230801_002711-t55c7qcz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matheusalb/huggingface/runs/t55c7qcz' target=\"_blank\">resilient-moon-72</a></strong> to <a href='https://wandb.ai/matheusalb/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matheusalb/huggingface' target=\"_blank\">https://wandb.ai/matheusalb/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matheusalb/huggingface/runs/t55c7qcz' target=\"_blank\">https://wandb.ai/matheusalb/huggingface/runs/t55c7qcz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520015562aff415fb5eeba3a8213fd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9319, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.3393, 'learning_rate': 0.0002, 'epoch': 0.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fdc51388d249e0bfbcc1645f4e1f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.135132908821106, 'eval_runtime': 31.3829, 'eval_samples_per_second': 6.883, 'eval_steps_per_second': 1.721, 'epoch': 0.18}\n",
      "{'loss': 1.1369, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 1.0129, 'learning_rate': 0.0002, 'epoch': 0.37}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6f0326e40d49ce9dcb2f55b1754da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.936887800693512, 'eval_runtime': 31.8801, 'eval_samples_per_second': 6.775, 'eval_steps_per_second': 1.694, 'epoch': 0.37}\n",
      "{'loss': 0.9629, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.9233, 'learning_rate': 0.0002, 'epoch': 0.55}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6d865e948c46eabcc9647fd81aceaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8547875881195068, 'eval_runtime': 31.9615, 'eval_samples_per_second': 6.758, 'eval_steps_per_second': 1.69, 'epoch': 0.55}\n",
      "{'loss': 0.8561, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 0.8309, 'learning_rate': 0.0002, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0b0f51d4224e82bad84a3067968eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8276867866516113, 'eval_runtime': 31.7902, 'eval_samples_per_second': 6.795, 'eval_steps_per_second': 1.699, 'epoch': 0.74}\n",
      "{'loss': 0.7898, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 0.8274, 'learning_rate': 0.0002, 'epoch': 0.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144ac7be052f43dab3a9a0bbbf330d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8160731196403503, 'eval_runtime': 27.2516, 'eval_samples_per_second': 7.926, 'eval_steps_per_second': 1.982, 'epoch': 0.92}\n",
      "{'train_runtime': 969.58, 'train_samples_per_second': 1.783, 'train_steps_per_second': 0.111, 'train_loss': 1.0435464558777985, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/qlora_LLaMA_2_7B_transformer_trainer2023-08-01T00:26:37.257542/tokenizer/tokenizer_config.json',\n",
       " './results/qlora_LLaMA_2_7B_transformer_trainer2023-08-01T00:26:37.257542/tokenizer/special_tokens_map.json',\n",
       " './results/qlora_LLaMA_2_7B_transformer_trainer2023-08-01T00:26:37.257542/tokenizer/tokenizer.model',\n",
       " './results/qlora_LLaMA_2_7B_transformer_trainer2023-08-01T00:26:37.257542/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(OUTPUT_DIR+'/trainer_save_model')\n",
    "tokenizer.save_pretrained(OUTPUT_DIR+'/tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_input(comment):\n",
    "    prompt = f'''Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
    "### Input:\n",
    "{comment}\n",
    "### Response:\n",
    "'''           \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config=model.generation_config\n",
    "generation_config.max_new_tokens=300\n",
    "# generation_config.temeperature=0.7\n",
    "# generation_config.top_k=0.9\n",
    "generation_config.num_return_sequences=1\n",
    "# generation_config.pad_token_id=tokenizer.eos_token_id\n",
    "# generation_config.eos_token_id=tokenizer.eos_token_id\n",
    "\n",
    "def inference(model, text):\n",
    "    enconded = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        input_ids=enconded.input_ids,\n",
    "        attention_mask=enconded.attention_mask,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1448: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Alimentação caríssima para um péssimo atendimento.. super mal atendido\n",
      "### Response:\n",
      "Caro cliente, lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo atendimento insatisfatório e pela alta cobrança. Vamos revisar nossos processos internos para garantir que isso não aconteça novamente. Agradecemos seu feedback e esperamos ter a oportunidade de oferecer uma experiência melhor no futuro.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Filé estava duro. Ficamos decepcionados.\n",
      "### Response:\n",
      "Caro cliente, lamentamos muito pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo filé que estava duro. Vamos investigar o ocorrido e tomar as medidas necessárias para garantir que isso não aconteça novamente. Agradecemos seu feedback e esperamos ter a oportunidade de recebê-lo novamente em nosso estabelecimento.\n"
     ]
    }
   ],
   "source": [
    "#treino\n",
    "comment='Filé estava duro. Ficamos decepcionados.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "A falta de vaga para cadeirantes e idosos é uma falha terrível nos dias de hoje, além de ser lei não tivemos nenhum apoio ou mesmo interesse por parte dos funcionários para ajudar na questão do cadeirante que estava nos acompanhado.\n",
      "Não vou deixar de avaliar os outros pontos, no que tange ao atendimento interno e as delícias que são servidas. tudo de bom. \n",
      "Mas não vá com cadeirante que passará sufoco.\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback e lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pela falta de vagas para cadeirantes e idosos e pela falta de apoio dos nossos funcionários para ajudar na questão do cadeirante. Vamos revisar nossos processos internos para garantir que isso não aconteça novamente. Quanto ao atendimento interno e às delícias servidas, agradecemos por reconhecê-los. Agradecemos por nos informar sobre essa questão e esperamos ter a oportunidade de oferecer uma experiência melhor em uma próxima visita.\n"
     ]
    }
   ],
   "source": [
    "comment='''A falta de vaga para cadeirantes e idosos é uma falha terrível nos dias de hoje, além de ser lei não tivemos nenhum apoio ou mesmo interesse por parte dos funcionários para ajudar na questão do cadeirante que estava nos acompanhado.\n",
    "Não vou deixar de avaliar os outros pontos, no que tange ao atendimento interno e as delícias que são servidas. tudo de bom. \n",
    "Mas não vá com cadeirante que passará sufoco.'''\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "### Response:\n",
      "Caro cliente, lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo atendimento lento e pela confusão com o prato que você pediu. Vamos investigar o ocorrido para garantir que isso não se repita. Agradecemos seu feedback e esperamos ter a oportunidade de oferecer uma experiência melhor no futuro.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "### Response:\n",
      "Caro cliente, lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo atendimento lento e pela confusão com o prato que você pediu. Vamos investigar o ocorrido para garantir que isso não se repita. Agradecemos seu feedback e esperamos ter a oportunidade de oferecer uma experiência melhor no futuro.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback. Lamentamos que a temperatura do nosso restaurante não tenha sido adequada para sua comodidade. Vamos avaliar nossos sistemas de climatização para garantir que isso não aconteça novamente. Quanto à demora na preparação da comida, entendemos que isso pode ter causado uma má experiência. Vamos trabalhar para melhorar nossos processos de produção para garantir que isso não aconteça novamente. Agradecemos seu feedback e esperamos ter a oportunidade de recebê-la novamente em nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback e entendemos sua frustração em relação ao tempo de espera para ser atendido na área externa. Lamentamos que isso tenha ocorrido e estamos trabalhando para melhorar nossa organização para atender todos os clientes de forma equilibrada. Agradecemos seu apoio e esperamos ter a oportunidade de recebê-la novamente em nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "O pior yakisoba que eu já comi na vida! Vem numa caixa com a massa toda amassada e o molho é horrível! Tive que jogar fora! Parecia comida de cachorro! Nunca mais!\n",
      "### Response:\n",
      "Caro cliente, lamentamos profundamente pela experiência negativa que você teve com nosso yakisoba. Pedimos desculpas pelo péssimo sabor do molho e pela massa amassada. Vamos revisar nossos processos para garantir que isso não aconteça novamente. Agradecemos seu feedback e esperamos ter a oportunidade de oferecer uma experiência melhor no futuro.\n"
     ]
    }
   ],
   "source": [
    "comment = 'O pior yakisoba que eu já comi na vida! Vem numa caixa com a massa toda amassada e o molho é horrível! Tive que jogar fora! Parecia comida de cachorro! Nunca mais!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Cheguei para tomar café as 7h da manhã, pouquíssimas pessoas ,pedi um omelete levou 23 minutos a essa altura meu café já havia acabado, e chegou uma pessoa depois de mim e o pedido dela que era muito mais que o meu simples omelete saiu primeiro.\n",
      "Péssimo atendimento .\n",
      "Já fui bem atendida por outros  funcionários  mais essa turma da manhã 3 pessoas nota 2.\n",
      "### Response:\n",
      "Caro cliente, lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo atendimento insatisfatório e pela demora no preparo do seu omelete. Vamos investigar o ocorrido e tomar as medidas necessárias para garantir que isso não se repita. Agradecemos seu feedback e esperamos ter a oportunidade de oferecer uma experiência melhor no futuro.\n"
     ]
    }
   ],
   "source": [
    "comment= '''Cheguei para tomar café as 7h da manhã, pouquíssimas pessoas ,pedi um omelete levou 23 minutos a essa altura meu café já havia acabado, e chegou uma pessoa depois de mim e o pedido dela que era muito mais que o meu simples omelete saiu primeiro.\n",
    "Péssimo atendimento .\n",
    "Já fui bem atendida por outros  funcionários  mais essa turma da manhã 3 pessoas nota 2.'''\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Não recomendo! Não faz muito meu estilo cantar para quem não conheço, coisa que é impossível sem marcar antes. Sim, existem vários ambientes tocando estilos diferentes mas é necessário muita sorte para conseguir um lugar onde todos gostem do que está tocando. Sem falar que, se tratando do karaokê: ninguém é profissional por isso as performances são horríveis e a musica e letra são retiradas do YouTube, mas mesmo assim o espaço insiste em cobrar entrada! Para finalizar a comida, que é bem cara, veio fria e não gostei de nada que comi! Nem a bebida salvou minha noite, paguei um absurdo pela menor porção de caipirinha que já vi! Teria feito melhor na minha casa!\n",
      "### Response:\n",
      "Caro cliente,\n",
      "\n",
      "Lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pela falta de consideração em relação ao seu estilo de cantar para quem não conhece. Reconhecemos que é possível que vários ambientes toquem estilos diferentes, mas não é possível garantir que todos gostarão do que está tocando.\n",
      "\n",
      "Quanto à questão do karaokê, entendemos sua frustração em relação às performances horríveis e às músicas e letras retiradas do YouTube. Vamos revisar nossos processos para garantir que isso não aconteça novamente.\n",
      "\n",
      "Quanto à comida, entendemos sua frustração em relação às porções fria e a bebida. Vamos revisar nossos processos de preparo para garantir que a comida seja servida à temperatura correta.\n",
      "\n",
      "Quanto à bebida, entendemos sua frustração em relação à porção pequena da caipirinha. Vamos revisar nossos preços para garantir que a bebida seja proporcional ao preço pago.\n",
      "\n",
      "Agradecemos seu feedback e vamos trabalhar\n"
     ]
    }
   ],
   "source": [
    "#treino\n",
    "comment='Não recomendo! Não faz muito meu estilo cantar para quem não conheço, coisa que é impossível sem marcar antes. Sim, existem vários ambientes tocando estilos diferentes mas é necessário muita sorte para conseguir um lugar onde todos gostem do que está tocando. Sem falar que, se tratando do karaokê: ninguém é profissional por isso as performances são horríveis e a musica e letra são retiradas do YouTube, mas mesmo assim o espaço insiste em cobrar entrada! Para finalizar a comida, que é bem cara, veio fria e não gostei de nada que comi! Nem a bebida salvou minha noite, paguei um absurdo pela menor porção de caipirinha que já vi! Teria feito melhor na minha casa!'\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Eu gosto muito de tomar café aos domingos nessa panificadora, porém não sei o que esta acontecendo, ela está ficando muito suja.\n",
      "Gosto da parte de cima,mas quando chego as mesas estão todas sujas, com xícaras, resto de lanches e etc.\n",
      "Acredito que não estão higienizando.\n",
      "Outro detalhe a tapioca está vindo muito salgada.\n",
      "Espero que melhore, para que possamos continuar frequentando.\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback sobre a limpeza da nossa panificadora. Lamentamos que você tenha tido uma experiência negativa em relação à higiene. Vamos revisar nossos processos de limpeza para garantir que todos os pontos da nossa estabelecimento sejam limpos e seguros. Agradecemos seu apoio e esperamos recebê-la novamente em nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "#treino\n",
    "comment = '''Eu gosto muito de tomar café aos domingos nessa panificadora, porém não sei o que esta acontecendo, ela está ficando muito suja.\n",
    "Gosto da parte de cima,mas quando chego as mesas estão todas sujas, com xícaras, resto de lanches e etc.\n",
    "Acredito que não estão higienizando.\n",
    "Outro detalhe a tapioca está vindo muito salgada.\n",
    "Espero que melhore, para que possamos continuar frequentando.'''\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Eu gostaria de deixar minha indignação quanto o atendimento de um garçom.\n",
      "Não estou  reclamando do restaurante.\n",
      "Estive no restaurante umas duas vezes, foi tudo muito bom.\n",
      "Mas me decepcionei na minha ultima vez.\n",
      "Foi atendida por um garçom mal educado e sem ética pois o mesmo alem de ser Grosso ficava olhando para as minhas pernas. Fiquei muito constrangida com a situação.\n",
      "Almoçamos tomamos. Sucos e cervejas, mesmo porque estavamos em 6 pessoas.\n",
      "No final pedimos a conta e o mesmo  trouxe duas contas uma com mais itens incluindo a refeiçao, outra somente com as bebidas,achei estranho porque tinham me cobrado normal das últimas vezes. Em fim.\n",
      "Como um gestor de um restaurante contrata pessoas sem conhecer seu carater?\n",
      "Tudo bem que a mão de obra de estrangeiros seja mais barata. Mas vocês teriam que pensar no bem estar dos clientes.\n",
      "Descobri o nome do garçom.\n",
      "CARLOS AMED. Ele é cubano.\n",
      "Ia fazer uma denuncia por assedio mas pensei na empresa. Por favor selecione melhor seus colaboradores.\n",
      "Muito indignada.\n",
      "### Response:\n",
      "Caro cliente, lamentamos profundamente que sua experiência no nosso restaurante tenha sido comprometida por um atendimento inadequado de um de nosso garçons. Reconhecemos que isso não cumpriu com nossos padrões de serviço e entendemos sua indignação.\n",
      "\n",
      "Agradecemos por compartilhar sua opinião conosco e por nos informar sobre o nome do garçom envolvido. Vamos investigar imediatamente e tomar as medidas necessárias para garantir que isso não aconteça novamente.\n",
      "\n",
      "Agradecemos por nos informar sobre a conta dupla e entendemos sua frustração. Vamos revisar nossos processos de contabilidade para evitar que isso ocorra novamente.\n",
      "\n",
      "Levamos a sério a importância do bem-estar dos nossos clientes e estamos comprometidos em oferecer uma experiência agradável e segura. Agradecemos novamente por nos informar sobre essa questão e esperamos ter a oportunidade de recebê-lo novamente em nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "comment = '''Eu gostaria de deixar minha indignação quanto o atendimento de um garçom.\n",
    "Não estou  reclamando do restaurante.\n",
    "Estive no restaurante umas duas vezes, foi tudo muito bom.\n",
    "Mas me decepcionei na minha ultima vez.\n",
    "Foi atendida por um garçom mal educado e sem ética pois o mesmo alem de ser Grosso ficava olhando para as minhas pernas. Fiquei muito constrangida com a situação.\n",
    "Almoçamos tomamos. Sucos e cervejas, mesmo porque estavamos em 6 pessoas.\n",
    "No final pedimos a conta e o mesmo  trouxe duas contas uma com mais itens incluindo a refeiçao, outra somente com as bebidas,achei estranho porque tinham me cobrado normal das últimas vezes. Em fim.\n",
    "Como um gestor de um restaurante contrata pessoas sem conhecer seu carater?\n",
    "Tudo bem que a mão de obra de estrangeiros seja mais barata. Mas vocês teriam que pensar no bem estar dos clientes.\n",
    "Descobri o nome do garçom.\n",
    "CARLOS AMED. Ele é cubano.\n",
    "Ia fazer uma denuncia por assedio mas pensei na empresa. Por favor selecione melhor seus colaboradores.\n",
    "Muito indignada.'''\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Fui no dia 10/12/21 às 11:00 o restaurante não tinha ninguém ainda, fui fazer um pedido para viagem. Depois de quase 30 minutos de espera percebo que tinham clientes que chegaram depois comendo...questionei um garçom pela demora do pedido foi averiguar e não deu nenhuma satisfação. Aí percebi depois que o pedido estava no balcão aguardando o favor de algum garçom pegar e olha que nem estava lotado o restaurante...desrespeito com o cliente e garçons mal treinados.. nunca mais nesse lugar.\n",
      "### Response:\n",
      "Caro cliente, lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pela demora no entrega do seu pedido e pela falta de atenção dos nossos garçons. Vamos revisar nossos processos internos para garantir que isso não aconteça novamente. Agradecemos seu feedback e esperamos ter a oportunidade de recebê-lo novamente em nosso estabelecimento.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Fui no dia 10/12/21 às 11:00 o restaurante não tinha ninguém ainda, fui fazer um pedido para viagem. Depois de quase 30 minutos de espera percebo que tinham clientes que chegaram depois comendo...questionei um garçom pela demora do pedido foi averiguar e não deu nenhuma satisfação. Aí percebi depois que o pedido estava no balcão aguardando o favor de algum garçom pegar e olha que nem estava lotado o restaurante...desrespeito com o cliente e garçons mal treinados.. nunca mais nesse lugar.\"\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Só não dou zero porque não dá e porque a torta de frango estava boazinha. Priorizam quem tem aparência de ter mais dinheiro e os garçons não fazem questão alguma de atender as pessoas. Olharam na minha cara, passaram direto até que eu implorasse pra ser atendida. :) Não anotaram meu pedido e ainda queriam colocar a culpa em mim pelo esquecimento. Enfim, não volto!\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback e lamentamos que sua experiência não tenha sido satisfatória. Pedimos desculpas pela falta de atenção e pela falta de respeito aos clientes. Vamos revisar nossos processos internos para garantir que isso não aconteça novamente. Quanto ao pedido, entendemos sua frustração e vamos investigar o ocorrido para garantir que isso não aconteça novamente. Agradecemos por nos informar e esperamos ter a oportunidade de recebê-la novamente em nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Só não dou zero porque não dá e porque a torta de frango estava boazinha. Priorizam quem tem aparência de ter mais dinheiro e os garçons não fazem questão alguma de atender as pessoas. Olharam na minha cara, passaram direto até que eu implorasse pra ser atendida. :) Não anotaram meu pedido e ainda queriam colocar a culpa em mim pelo esquecimento. Enfim, não volto!\"\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Pedimos um prato de bife ao molho madeira sendo que o feijão veio com gosto de molho madeira e o bife veio com gosto de queijo então eu recomendaria que vocês se dedicassem mais e as atende vistas são muito de humor baixo\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback sobre a sua experiência em nosso restaurante. Lamentamos muito que o prato de bife ao molho madeira não tenha atendido às suas expectativas. Vamos revisar nossos processos para garantir que isso não aconteça novamente. Quanto às atende vistas, vamos trabalhar para melhorar a qualidade do nosso serviço. Agradecemos por nos informar e esperamos ter a oportunidade de recebê-la novamente em nosso estabelecimento.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Pedimos um prato de bife ao molho madeira sendo que o feijão veio com gosto de molho madeira e o bife veio com gosto de queijo então eu recomendaria que vocês se dedicassem mais e as atende vistas são muito de humor baixo\"\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Escreva, em Português, um comentário de resposta ao seguinte comentário de um cliente ao seu restaurante\n",
      "### Input:\n",
      "Pedi no ifood camarão empanado e mandaram camarão alho e óleo muito sem graça e poucas unidades. Fui inventar de experimentar por causa da propaganda no Instagram e me ferrei! Não recomendo.\n",
      "### Response:\n",
      "Olá, agradecemos pelo seu feedback. Lamentamos muito pela experiência negativa que você teve com nosso camarão empanado. Vamos revisar nossos processos para garantir que isso não aconteça novamente. Agradecemos por nos informar e esperamos ter a oportunidade de recebê-la novamente em nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Pedi no ifood camarão empanado e mandaram camarão alho e óleo muito sem graça e poucas unidades. Fui inventar de experimentar por causa da propaganda no Instagram e me ferrei! Não recomendo.\"\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
