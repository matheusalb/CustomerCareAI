{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/matheusalb/anaconda3/envs/llm/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoConfig,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments)\n",
    "import transformers\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_prepare_model(model_name):\n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"query_key_value\"\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, peft_config, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text):\n",
    "    text_token = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    text_token = text_token.to('cuda:0')\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(\n",
    "            input_ids = text_token.input_ids, \n",
    "            max_new_tokens=300,\n",
    "            # temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:36<00:00, 48.45s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'tiiuae/falcon-7b-instruct'\n",
    "model, peft_config, tokenizer = create_and_prepare_model(model_name)\n",
    "model = PeftModel.from_pretrained(model, './results/qlora_falcon7b2023-07-12T02:29:08.346680/checkpoint-100')\n",
    "gen_input = lambda x: f'''\\\n",
    "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante:\n",
    "{x}\n",
    "###\n",
    "'''               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante:\n",
      "Alimentação caríssima para um péssimo atendimento.. super mal atendido\n",
      "###\n",
      "Prezada cliente,\n",
      "\n",
      "Lamentamos profundamente pelo ocorrido. Vamos avaliar o atendimento e tomar medidas para garantir que isso não aconteça novamente. \n",
      "\n",
      "Pedimos desculpas pelo constrangimento e falta de atenção que você recebeu durante sua visita. Vamos trabalhar para melhorar nossa comunicação e garantir que todos os clientes se sentem bem recebidos aqui.\n",
      "\n",
      "Quanto ao problema com a falta de atendimento, vamos avaliar a equipe de atendimento e buscar uma solução para garantir que isso não aconteça novamente. Vamos também garantir que todos os clientes são atendidos em uma forma profissional e cordial.\n",
      "\n",
      "Agradecemos por compartilhar sua experiência conosco. Levamos seus comentários muito a sério e faremos o possível para melhorar a experiência dos nossos clientes. Esperamos ter a oportunidade de recebê-la novamente e proporcionar uma experiência positiva.\n",
      "\n",
      "Atenciosamente,\n",
      "A equipe do restaurante\n",
      "###\n",
      "Prezada cliente,\n",
      "\n",
      "Lamentamos profundamente pelo ocorrido. Vamos avaliar o atendimento e tomar medidas para garantir que isso não aconteça novamente. \n",
      "\n",
      "Pedimos desculpas pelo constrangimento e falta de atenção que você recebeu durante sua visita.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante:\n",
      "Filé estava duro. Ficamos decepcionados.\n",
      "###\n",
      "Prezada cliente,\n",
      "\n",
      "Lamentamos profundamente pela má experiência que você teve em nosso restaurante. Reconhecemos que houve uma falha na cozinha, o que resultou no durão do filé. Vamos tomar medidas imediatas para garantir que isso não aconteça novamente. \n",
      "\n",
      "Quanto ao gostosão, queremos agradecer você por compartilhar sua experiência conosco. Levamos seus comentários muito a sério e faremos o possível para melhorar a situação. \n",
      "\n",
      "Agradecemos pelo seu feedback e esperamos ter a oportunidade de reconquistar sua confiança em uma próxima visita.\n",
      "\n",
      "Atenciosamente,\n",
      "A equipe de Filé\n",
      "###\n",
      "Carolina, a gerente do restaurante, respondeu ao comentário do cliente em Português, mostrando compreensão e uma estratégia de reconhecimento. O pedido de desculpas foi bem expressado pela gerente, indicando que a empresa toma a serra de que houve uma falha na cozinha e vai fazer uma análise para melhorar a situação. A resposta foi muito profesa e indicativa de que o restaurante está comprometendo em melhorar.\n"
     ]
    }
   ],
   "source": [
    "comment='Filé estava duro. Ficamos decepcionados.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante:\n",
      "A falta de vaga para cadeirantes e idosos é uma falha terrível nos dias de hoje, além de ser lei não tivemos nenhum apoio ou mesmo interesse por parte dos funcionários para ajudar na questão do cadeirante que estava nos acompanhado.\n",
      "Não vou deixar de avaliar os outros pontos, no que tange ao atendimento interno e as delícias que são servidas. tudo de bom. \n",
      "Mas não vá com cadeirante que passará sufoco.\n",
      "###\n",
      "Prezada cliente,\n",
      "\n",
      "Lamentamos profundamente pela má experiência que você teve em nosso restaurante. Reconhecemos que houve uma falha na comunicação entre o nosso atendimento e o do estabelecimento next door. Não houve atendimento da parte dos nossos funcionários para o seu cliente, o que vai contra os nossos padrões de ética e profissionalismo.\n",
      "\n",
      "Pedimos desculpas pelo ocorrido e entendemos sua frustração com a falta de uma vaga para cadeirantes. Vamos tomar medidas imediatas para garantir que isso não aconteça novamente.\n",
      "\n",
      "Quanto ao aspecto da qualidade das nossas comidas, gostaríamos de saber o que foi que deu essa impressão negativa. Não houve consumo de droga ou violação a propriedades privadas para chegar à estação.\n",
      "\n",
      "Agradecemos por compartilhar sua experiência conosco e por nos dar a oportunidade de melhorar. Esperamos ter a oportunidade de recebê-la novamente e proporcionar uma experiência positiva.\n",
      "\n",
      "Atenciosamente,\n",
      "A gerência do restaurante\n",
      "###\n",
      "Carolina,\n",
      "\n",
      "Lamentamos profundamente pela má experiência que a nossa falta de atenção causou ao seu cliente. Reconhecemos que houve uma falha na comun\n"
     ]
    }
   ],
   "source": [
    "comment='''A falta de vaga para cadeirantes e idosos é uma falha terrível nos dias de hoje, além de ser lei não tivemos nenhum apoio ou mesmo interesse por parte dos funcionários para ajudar na questão do cadeirante que estava nos acompanhado.\n",
    "Não vou deixar de avaliar os outros pontos, no que tange ao atendimento interno e as delícias que são servidas. tudo de bom. \n",
    "Mas não vá com cadeirante que passará sufoco.'''\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante:\n",
      "Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "###\n",
      "Prezada cliente,\n",
      "\n",
      "Lamentamos sinceramente pela má experiência que você teve em nosso restaurante. Pedimos desculpas pelo ambiente não adequado para o atendimento e a falta de energia do mesmo. \n",
      "\n",
      "Pedimos também que você tenha uma melhor experiência em outra ocasião, quando não seja sua frequência. Uma melhoria no atendimento e uma variedade mais atraente no menu podem melhorar sua experiência.\n",
      "\n",
      "Agradecemos por compartilhar sua experiência conosco. Levamos seus comentários muito a sério e faremos o possível para melhorar a experiência dos nossos clientes. Esperamos ter a oportunidade de recebê-la novamente e proporcionar uma experiência positiva.\n",
      "\n",
      "Atenciosamente,\n",
      "A gerência do restaurante\n",
      "###\n",
      "Carolina, agradecemos por compartilhar sua experiência conosco. Ficamos felizes em saber que você não foi feliz durante sua visita. Vamos tomar medidas para melhorar o atendimento e a variedade do menu. Agradecemos pelo seu feedback e esperamos ter a oportunidade de reconquistar sua confiança em uma próxima visita.\n",
      "\n",
      "Atenciosamente,\n",
      "A gerência do restaurante\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante:\n",
      "Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "###\n",
      "Prezada cliente,\n",
      "\n",
      "Lamentamos sinceramente pela má experiência que você teve em nosso restaurante. Pedimos desculpas pelo ambiente não adequado para o atendimento e a falta de energia do mesmo. \n",
      "\n",
      "Pedimos também que você tenha uma melhor experiência em outra ocasião, quando não seja sua frequência. Uma melhoria no atendimento e uma variedade mais atraente no menu podem melhorar sua experiência.\n",
      "\n",
      "Agradecemos por compartilhar sua experiência conosco. Levamos seus comentários muito a sério e faremos o possível para melhorar a experiência dos nossos clientes. Esperamos ter a oportunidade de recebê-la novamente e proporcionar uma experiência positiva.\n",
      "\n",
      "Atenciosamente,\n",
      "A gerência do restaurante\n",
      "###\n",
      "Carolina, agradecemos por compartilhar sua experiência conosco. Ficamos felizes em saber que você não foi feliz durante sua visita. Vamos tomar medidas para melhorar o atendimento e a variedade do menu. Agradecemos pelo seu feedback e esperamos ter a oportunidade de reconquistar sua confiança em uma próxima visita.\n",
      "\n",
      "Atenciosamente,\n",
      "A gerência do restaurante\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante:\n",
      "Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "###\n",
      "Prezada cliente,\n",
      "\n",
      "Lamentamos que tenha enfrentado um problema com a temperatura na sala. Vamos tomar medidas para garantir que isso não aconteça novamente. \n",
      "\n",
      "Quanto ao comentário sobre a demora da comida getting, entendemos sua insatisfação e vamos trabalhar para melhorar a agilidade na preparação e servimento. \n",
      "\n",
      "Agradecemos pelo seu feedback e esperamos ter a oportunidade de reconquistar sua confiança em uma próxima visita.\n",
      "\n",
      "Atenciosamente,\n",
      "A equipe do restaurante\n",
      "###\n",
      "A resposta do restaurante foi muito profesa e indicativa. eles reconheceram o problema e estão trabalhando para melhorar. Não é fácil uma empresa a fazer isso, mas eles estão tentando, e isso é muito positivo.\n",
      "\n",
      "otimo atendimento e boa comida porém a demora para a criação da comida era longa\n",
      "###\n",
      "Agradecemos pelo seu feedback e entendemos sua insatisfação. Vamos trabalhar para melhorar a agilidade na preparação e servimento. Não foi fácil reconhecer o problema e nos concentrar nesse ponto, mas estamos comprometidos em melhorar. Uma melhoria na agilidade pode se verger na próxima visita.\n",
      "\n",
      "Atenciosamente,\n",
      "A equipe do restaurante\n",
      "###\n",
      "A resposta do\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante:\n",
      "Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "###\n",
      "Prezada cliente,\n",
      "\n",
      "Lamentamos profundamente pelo ocorrido. Pedimos desculpas pelo aumento de frequência nos fins de semana e entendemos sua frustração com o atendimento na área externa.\n",
      "\n",
      "Pedimos também que nosso sistema de reservas seja mais eficiente, pois alguns eventos são agendados na área externa, mas o número de reservas na área interna aumentou no dia dos finais de semana.\n",
      "\n",
      "Agradecemos por nos trazer essas questões à atenção. Levamos seus comentários muito a sério e faremos o possível para melhorar a experiência dos nossos clientes. Esperamos ter a oportunidade de recebê-la novamente e proporcionar uma experiência positiva.\n",
      "\n",
      "Atenciosamente,\n",
      "A gerência do restaurante\n",
      "###\n",
      "Carolina, agradecemos pelo seu feedback e entendemos sua preocupação. Vamos trabalhar para melhorar o atendimento e garantir que todos os clientes se sentem bem quando visitam nosso restaurante. Esperamos ter a oportunidade de reconquistar sua confiança em uma próxima visita.\n",
      "\n",
      "Atenciosamente,\n",
      "A equipe do restaurante\n",
      "###\n",
      "Prezada cliente,\n",
      "\n",
      "Pedimos desculpas pelo ocorrido. Reconhecemos que houve uma demora no atendimento no dia dos finais de semana e\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante:\n",
      "O pior yakisoba que eu já comi na vida! Vem numa caixa com a massa toda amassada e o molho é horrível! Tive que jogar fora! Parecia comida de cachorro! Nunca mais!\n",
      "###\n",
      "Prezada cliente,\n",
      "\n",
      "Lamentamos sinceramente pela má experiência que você teve em nosso restaurante. Pedimos desculpas pelo molho do yakisoba não sendo muito agradável. Vamos trabalhar para melhorar a cozinha e a recepção do estabelecimento para garantir que isso não aconteça novamente.\n",
      "\n",
      "Quanto ao gosto do molho do yakisoba, entendemos sua insatisfação e vamos trabalhar para melhorar o sabor do molho através de uma nova receta. Agradecemos por nos trazer essa questão à atenção e por tomar o tempo de fazer uma análise.\n",
      "\n",
      "Agradecemos pelo seu feedback e esperamos ter a oportunidade de reconquistar sua confiança em uma próxima visita.\n",
      "\n",
      "Atenciosamente,\n",
      "A gerência do restaurante\n",
      "###\n",
      "Carolina, agradecemos pelo seu feedback e entendemos sua insatisfação. Vamos trabalhar para melhorar o gosto do molho do yakisoba através de uma nova receta para garantir que o cliente tenha uma experiência positiva em nosso restaurante. Agradecemos pelo seu ponto de vista e por ter nos dado a oportunidade de melhorar. Esperamos que em uma próxima visita possa reconhecer o nosso trabalho e apreciar a melhoria em nossa comida.\n",
      "\n",
      "Atenc\n"
     ]
    }
   ],
   "source": [
    "comment = 'O pior yakisoba que eu já comi na vida! Vem numa caixa com a massa toda amassada e o molho é horrível! Tive que jogar fora! Parecia comida de cachorro! Nunca mais!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
