{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/matheusalb/anaconda3/envs/llm/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from utils.create_and_prepare_model_qlora import (\n",
    "    create_and_prepare_model,\n",
    "    create_and_prepare_model_FALCON\n",
    ")\n",
    "\n",
    "import torch\n",
    "# import bitsandbytes as bnb\n",
    "import datetime\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset \n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    messages = {}\n",
    "    messages['comentario'] = []\n",
    "    messages['sugestaoResposta'] = [] \n",
    "    \n",
    "    df = pd.read_csv(path, skip_blank_lines=True)\n",
    "    df = df.dropna(how='all')\n",
    "    for _, linha in df.iterrows():\n",
    "        try:\n",
    "            int(linha['idSugestaoResposta'])\n",
    "        except:\n",
    "            print(linha)\n",
    "        messages['comentario'].append(linha['comentario'])\n",
    "        messages['sugestaoResposta'].append(linha['sugestaoResposta'])\n",
    "    \n",
    "    return Dataset.from_dict(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    prompt = f'''\\\n",
    "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
    "{data_point['comentario']}\n",
    "<assistant>: {data_point['sugestaoResposta']}\n",
    "'''.strip() + EOS_TOKEN\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # ## adicionando eos token ao final dos tokens\n",
    "    # if (\n",
    "    #     result['input_ids'][-1] != tokenizer.eos_token_id\n",
    "    #     and len(result['input_ids']) < MAX_LENGTH\n",
    "    #     and ADD_EOS_TOKEN\n",
    "    # ):\n",
    "    #     result['input_ids'].append(tokenizer.eos_token_id)\n",
    "    #     result['attention_mask'].append(1)\n",
    "        \n",
    "    # # generative models: labels are the same as the input\n",
    "    # result['labels'] = result['input_ids'].copy()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Tokenize the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_prompt = tokenize(full_prompt)\n",
    "    return tokenized_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tiiuae/falcon-7b'\n",
    "train_path = '../../../data/base_2k/train_base.csv'\n",
    "validation_path = '../../../data/base_2k/validation_base.csv'    \n",
    "OUTPUT_DIR = \"./results/qlora_falcon7b_2k_trainer_\"+datetime.datetime.now().isoformat()\n",
    "\n",
    "MAX_LENGTH=350\n",
    "ADD_EOS_TOKEN=True\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "peft_config =  LoraConfig(\n",
    "        lora_alpha=16, # 16\n",
    "        lora_dropout=0.1, # 0.05\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=64, # 8\n",
    "        target_modules=[\n",
    "            \"query_key_value\",\n",
    "            \"dense\",\n",
    "            \"dense_h_to_4h\",\n",
    "            \"dense_4h_to_h\",\n",
    "        ]\n",
    ")\n",
    "\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,      # batch_size = 16\n",
    "    per_device_train_batch_size=2, # micro_batch_size\n",
    "    gradient_accumulation_steps=8, # batch_size / micro_batch_size\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    # max_steps=100,\n",
    "    # save_steps=648,\n",
    "    optim=\"paged_adamw_32bit\", # adamw_torch\n",
    "    # save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4, # 3e-4\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03, #0.05\n",
    "    # warmup_steps=100,\n",
    "    lr_scheduler_type=\"constant\", #cosine\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=20,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 130,547,712 || all params: 3,739,297,088 || trainable%: 3.4912366930926244\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = create_and_prepare_model_FALCON(model_name, peft_config, bnb_config)\n",
    "model.config.use_cache = False\n",
    "model.print_trainable_parameters()\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_to_multiple_of -> An integer representing the maximum sequence length, rounded up to the nearest multiple of this value.\n",
    "# data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "#     tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "# )\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "train_data = get_dataset(train_path)\n",
    "validation_data = get_dataset(validation_path)\n",
    "\n",
    "train_data = train_data.map(generate_and_tokenize_prompt)\n",
    "validation_data = validation_data.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatheusalb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matheusalb/Documents/CustomerCareAI/notebooks/3-Finetuning/Falcon7B/wandb/run-20230723_192716-e438vth6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matheusalb/huggingface/runs/e438vth6' target=\"_blank\">worthy-water-67</a></strong> to <a href='https://wandb.ai/matheusalb/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matheusalb/huggingface' target=\"_blank\">https://wandb.ai/matheusalb/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matheusalb/huggingface/runs/e438vth6' target=\"_blank\">https://wandb.ai/matheusalb/huggingface/runs/e438vth6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/108 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  9%|▉         | 10/108 [02:14<19:01, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9016, 'learning_rate': 0.0002, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 20/108 [04:27<20:09, 13.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2067, 'learning_rate': 0.0002, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 19%|█▊        | 20/108 [05:32<20:09, 13.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1062461137771606, 'eval_runtime': 65.2, 'eval_samples_per_second': 3.313, 'eval_steps_per_second': 1.656, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 30/108 [08:03<19:46, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1422, 'learning_rate': 0.0002, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 40/108 [10:29<16:24, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0567, 'learning_rate': 0.0002, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 37%|███▋      | 40/108 [11:39<16:24, 14.48s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.013821005821228, 'eval_runtime': 70.0711, 'eval_samples_per_second': 3.083, 'eval_steps_per_second': 1.541, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 50/108 [14:08<14:56, 15.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0341, 'learning_rate': 0.0002, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 60/108 [16:39<12:01, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0236, 'learning_rate': 0.0002, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 56%|█████▌    | 60/108 [17:46<12:01, 15.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9812086820602417, 'eval_runtime': 67.1422, 'eval_samples_per_second': 3.217, 'eval_steps_per_second': 1.609, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 70/108 [20:08<09:20, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9713, 'learning_rate': 0.0002, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 80/108 [21:26<03:25,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9703, 'learning_rate': 0.0002, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 74%|███████▍  | 80/108 [22:00<03:25,  7.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.965034008026123, 'eval_runtime': 34.1895, 'eval_samples_per_second': 6.318, 'eval_steps_per_second': 3.159, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 90/108 [23:11<02:16,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9098, 'learning_rate': 0.0002, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 100/108 [24:28<01:03,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9577, 'learning_rate': 0.0002, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 93%|█████████▎| 100/108 [25:06<01:03,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9482447504997253, 'eval_runtime': 37.5076, 'eval_samples_per_second': 5.759, 'eval_steps_per_second': 2.879, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [26:10<00:00, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1574.958, 'train_samples_per_second': 1.098, 'train_steps_per_second': 0.069, 'train_loss': 1.1053268203028925, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=108, training_loss=1.1053268203028925, metrics={'train_runtime': 1574.958, 'train_samples_per_second': 1.098, 'train_steps_per_second': 0.069, 'train_loss': 1.1053268203028925, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "# model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(OUTPUT_DIR+'/trainer_save_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/qlora_falcon7b_2k_trainer_2023-07-23T19:25:11.275576/tokenizer/tokenizer_config.json',\n",
       " './results/qlora_falcon7b_2k_trainer_2023-07-23T19:25:11.275576/tokenizer/special_tokens_map.json',\n",
       " './results/qlora_falcon7b_2k_trainer_2023-07-23T19:25:11.275576/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(OUTPUT_DIR+'/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f2c820397840c5b890e83c611f03db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"matheusalb/finetuning_1_epoch_customercareai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input = lambda x: f'''\\\n",
    "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
    "{x}\n",
    "<assistant>:\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config=model.generation_config\n",
    "generation_config.use_cache = False\n",
    "generation_config.max_new_tokens=300\n",
    "# generation_config['eos_token_id'] = tokenizer.eos_token_id\n",
    "# generation_config['pad_token_id'] = tokenizer.pad_token_id\n",
    "# generation_config.temeperature=0.7\n",
    "# generation_config.top_k=0.9\n",
    "generation_config.num_return_sequences=1\n",
    "# generation_config.pad_token_id=tokenizer.eos_token_id\n",
    "# generation_config.eos_token_id=tokenizer.eos_token_id\n",
    "\n",
    "def inference(model, text):\n",
    "    enconded = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        input_ids=enconded.input_ids,\n",
    "        attention_mask=enconded.attention_mask,\n",
    "        # generation_config=generation_config,\n",
    "        max_new_tokens=350,\n",
    "        use_cache=False,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1448: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Alimentação caríssima para um péssimo atendimento.. super mal atendido\n",
      "<assistant>: Caro cliente, lamentamos profundamente que sua experiência em nosso restaurante tenha sido insatisfatória devido ao atendimento e à qualidade da comida. Agradecemos por compartilhar sua opinião e gostaríamos de entender melhor os pontos negativos mencionados para melhorar nossos serviços.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Alimentação caríssima para um péssimo atendimento.. super mal atendido'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Filé estava duro. Ficamos decepcionados.\n",
      "<assistant>: Caro cliente, lamentamos profundamente que o filé não tenha atendido às suas expectativas em relação à consistência. Agradecemos por nos informar sobre essa questão e vamos investigar o ocorrido para garantir que isso não se repita. Agradecemos seu feedback e esperamos ter a oportunidade de recebê-lo novamente em nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "#treino\n",
    "comment='Filé estava duro. Ficamos decepcionados.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "A falta de vaga para cadeirantes e idosos é uma falha terrível nos dias de hoje, além de ser lei não tivemos nenhum apoio ou mesmo interesse por parte dos funcionários para ajudar na questão do cadeirante que estava nos acompanhado.\n",
      "Não vou deixar de avaliar os outros pontos, no que tange ao atendimento interno e as delícias que são servidas. tudo de bom. \n",
      "Mas não vá com cadeirante que passará sufoco.\n",
      "<assistant>: Caro cliente, agradecemos por compartilhar sua experiência conosco. Lamentamos profundamente pela falta de vaga para cadeirantes e idosos em nosso estabelecimento. Reconhecemos que isso é uma falha terrível nos dias de hoje e estamos trabalhando para melhorar essa situação. Agradecemos seu feedback e vamos reforçar a importância de oferecer um atendimento adequado a todos os nossos clientes. Quanto à questão da cadeirante que estava acompanhando, pedimos desculpas pela falta de apoio e interesse dos nossos funcionários. Vamos reforçar a importância de um atendimento adequado a todos os nossos clientes, incluindo aos que se encontram em situações de exclusão. Quanto aos pontos positivos mencionados em sua avaliação, estamos felizes em saber que você gostou da comida e do atendimento interno. Esperamos ter a oportunidade de recebê-lo novamente em nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "comment='''A falta de vaga para cadeirantes e idosos é uma falha terrível nos dias de hoje, além de ser lei não tivemos nenhum apoio ou mesmo interesse por parte dos funcionários para ajudar na questão do cadeirante que estava nos acompanhado.\n",
    "Não vou deixar de avaliar os outros pontos, no que tange ao atendimento interno e as delícias que são servidas. tudo de bom. \n",
    "Mas não vá com cadeirante que passará sufoco.'''\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "<assistant>: Olá, agradecemos pelo seu feedback. Lamentamos muito pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo atendimento demorado e pelo frango com gosto de peixe. Vamos investigar o ocorrido e tomar as medidas necessárias para melhorar esses pontos negativos. Agradecemos por nos informar sobre sua experiência e esperamos ter a oportunidade de recebê-la novamente para uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!\n",
      "<assistant>: Olá, agradecemos pelo seu feedback. Lamentamos muito pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo atendimento demorado e pelo frango com gosto de peixe. Vamos investigar o ocorrido e tomar as medidas necessárias para melhorar esses pontos negativos. Agradecemos por nos informar sobre sua experiência e esperamos ter a oportunidade de recebê-la novamente para uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Horrível! Restaurante vazio e o atendimento ainda demora. Pedi um frango e veio com gosto de de peixe. Nojo!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.\n",
      "<assistant>: Caro cliente, agradecemos por compartilhar sua experiência conosco. Lamentamos que a temperatura do nosso estabelecimento tenha sido desconfortável e que a espera pela comida tenha sido longa. Vamos revisar nossos processos para garantir que isso não aconteça novamente. Agradecemos seu feedback e esperamos ter a oportunidade de recebê-lo novamente em nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Lugar estava quente e, apesar de poucos usuários, a comida demorou bastante a ficar pronta.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.\n",
      "<assistant>: Caro cliente, agradecemos por compartilhar sua experiência conosco. Lamentamos profundamente pela demora no atendimento em nosso restaurante. Vamos reforçar a comunicação entre nossos funcionários para que isso não aconteça novamente. Agradecemos seu feedback e esperamos ter a oportunidade de recebê-lo novamente em nosso estabelecimento.\n"
     ]
    }
   ],
   "source": [
    "comment = 'Com o aumento do público nos finais de semana, poderiam conciliar o atendimento na área interna com a área externa, pois quem fica do lado de fora espera cerca de 40 min pra ser atendido.'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "O pior yakisoba que eu já comi na vida! Vem numa caixa com a massa toda amassada e o molho é horrível! Tive que jogar fora! Parecia comida de cachorro! Nunca mais!\n",
      "<assistant>: Olá, agradecemos pelo seu feedback sobre o nosso yakisoba. Lamentamos muito que a sua experiência não tenha sido satisfatória. Vamos revisar nossos processos para garantir que isso não aconteça novamente. Agradecemos por nos informar sobre essa questão.\n"
     ]
    }
   ],
   "source": [
    "comment = 'O pior yakisoba que eu já comi na vida! Vem numa caixa com a massa toda amassada e o molho é horrível! Tive que jogar fora! Parecia comida de cachorro! Nunca mais!'\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Cheguei para tomar café as 7h da manhã, pouquíssimas pessoas,pedi um omelete levou 23 minutos a essa altura meu café já havia acabado, e chegou uma pessoa depois de mim e o pedido dela que era muito mais que o meu simples omelete saiu primeiro.\n",
      "Péssimo atendimento.\n",
      "Já fui bem atendida por outros  funcionários  mais essa turma da manhã 3 pessoas nota 2.\n",
      "<assistant>: Caro cliente, lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo atraso na entrega do seu pedido e pela demora no atendimento. Reconhecemos que isso não correspondeu às nossas expectativas e que não atendeu às suas expectativas em relação ao atendimento. Levaremos suas críticas em consideração para melhorar nosso serviço. Agradecemos por compartilhar sua opinião e esperamos ter a oportunidade de recebê-lo novamente para uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment= '''Cheguei para tomar café as 7h da manhã, pouquíssimas pessoas ,pedi um omelete levou 23 minutos a essa altura meu café já havia acabado, e chegou uma pessoa depois de mim e o pedido dela que era muito mais que o meu simples omelete saiu primeiro.\n",
    "Péssimo atendimento .\n",
    "Já fui bem atendida por outros  funcionários  mais essa turma da manhã 3 pessoas nota 2.'''\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Não recomendo! Não faz muito meu estilo cantar para quem não conheço, coisa que é impossível sem marcar antes. Sim, existem vários ambientes tocando estilos diferentes mas é necessário muita sorte para conseguir um lugar onde todos gostem do que está tocando. Sem falar que, se tratando do karaokê: ninguém é profissional por isso as performances são horríveis e a musica e letra são retiradas do YouTube, mas mesmo assim o espaço insiste em cobrar entrada! Para finalizar a comida, que é bem cara, veio fria e não gostei de nada que comi! Nem a bebida salvou minha noite, paguei um absurdo pela menor porção de caipirinha que já vi! Teria feito melhor na minha casa!\n",
      "<assistant>: Olá, agradecemos pelo seu feedback sobre a sua experiência em nosso restaurante. Lamentamos que a música e a letra tenham sido retiradas do YouTube, isso pode ter causado uma má impressão. Vamos revisar nossos processos para garantir que isso não aconteça novamente. Quanto à comida, pedimos desculpas pela qualidade insatisfatória e pela temperatura errada. Vamos investigar o ocorrido para evitar que isso aconteça novamente. Quanto à bebida, lamentamos que a porção não tenha atendido às suas expectativas. Vamos revisar nossos preços para garantir que estejam de acordo com a qualidade dos produtos. Agradecemos por nos informar sobre esses pontos negativos e esperamos ter a oportunidade de recebê-la novamente para uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "#treino\n",
    "comment='Não recomendo! Não faz muito meu estilo cantar para quem não conheço, coisa que é impossível sem marcar antes. Sim, existem vários ambientes tocando estilos diferentes mas é necessário muita sorte para conseguir um lugar onde todos gostem do que está tocando. Sem falar que, se tratando do karaokê: ninguém é profissional por isso as performances são horríveis e a musica e letra são retiradas do YouTube, mas mesmo assim o espaço insiste em cobrar entrada! Para finalizar a comida, que é bem cara, veio fria e não gostei de nada que comi! Nem a bebida salvou minha noite, paguei um absurdo pela menor porção de caipirinha que já vi! Teria feito melhor na minha casa!'\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Eu gosto muito de tomar café aos domingos nessa panificadora, porém não sei o que esta acontecendo, ela está ficando muito suja.\n",
      "Gosto da parte de cima,mas quando chego as mesas estão todas sujas, com xícaras, resto de lanches e etc.\n",
      "Acredito que não estão higienizando.\n",
      "Outro detalhe a tapioca está vindo muito salgada.\n",
      "Espero que melhore, para que possamos continuar frequentando.\n",
      "<assistant>: Olá, agradecemos pelo seu feedback sobre a nossa panificadora. Lamentamos muito pela sujeira e a falta de higienização. Vamos reforçar a importância da limpeza e higienização com nossa equipe. Quanto à tapioca, vamos revisar a preparação para garantir que esteja de acordo com as expectativas dos nossos clientes. Agradecemos por nos informar sobre esses pontos negativos e esperamos poder recebê-la novamente em breve.\n"
     ]
    }
   ],
   "source": [
    "#treino\n",
    "comment = '''Eu gosto muito de tomar café aos domingos nessa panificadora, porém não sei o que esta acontecendo, ela está ficando muito suja.\n",
    "Gosto da parte de cima,mas quando chego as mesas estão todas sujas, com xícaras, resto de lanches e etc.\n",
    "Acredito que não estão higienizando.\n",
    "Outro detalhe a tapioca está vindo muito salgada.\n",
    "Espero que melhore, para que possamos continuar frequentando.'''\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Eu gostaria de deixar minha indignação quanto o atendimento de um garçom.\n",
      "Não estou  reclamando do restaurante.\n",
      "Estive no restaurante umas duas vezes, foi tudo muito bom.\n",
      "Mas me decepcionei na minha ultima vez.\n",
      "Foi atendida por um garçom mal educado e sem ética pois o mesmo alem de ser Grosso ficava olhando para as minhas pernas. Fiquei muito constrangida com a situação.\n",
      "Almoçamos tomamos. Sucos e cervejas, mesmo porque estavamos em 6 pessoas.\n",
      "No final pedimos a conta e o mesmo  trouxe duas contas uma com mais itens incluindo a refeiçao, outra somente com as bebidas,achei estranho porque tinham me cobrado normal das últimas vezes. Em fim.\n",
      "Como um gestor de um restaurante contrata pessoas sem conhecer seu carater?\n",
      "Tudo bem que a mão de obra de estrangeiros seja mais barata. Mas vocês teriam que pensar no bem estar dos clientes.\n",
      "Descobri o nome do garçom.\n",
      "CARLOS AMED. Ele é cubano.\n",
      "Ia fazer uma denuncia por assedio mas pensei na empresa. Por favor selecione melhor seus colaboradores.\n",
      "Muito indignada.\n",
      "<assistant>: Caro cliente,\n",
      "\n",
      "Lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo comportamento inadequado do nosso garçom, que não refletiu a nossa política de respeito e ética.\n",
      "\n",
      "Também levamos seu feedback a sério e estamos investigando o ocorrido para garantir que isso não se repita.\n",
      "\n",
      "Agradecemos por nos informar sobre o nome do garçom e vamos tomar medidas imediatas para garantir que isso não se repita.\n",
      "\n",
      "Valorizamos a satisfação dos nossos clientes e estamos comprometidos em melhorar nosso serviço de atendimento.\n",
      "\n",
      "Esperamos ter a oportunidade de recebê-lo novamente em nosso restaurante e proporcionar uma experiência positiva.\n",
      "\n",
      "Atenciosamente,\n",
      "A equipe do restaurante\n"
     ]
    }
   ],
   "source": [
    "comment = '''Eu gostaria de deixar minha indignação quanto o atendimento de um garçom.\n",
    "Não estou  reclamando do restaurante.\n",
    "Estive no restaurante umas duas vezes, foi tudo muito bom.\n",
    "Mas me decepcionei na minha ultima vez.\n",
    "Foi atendida por um garçom mal educado e sem ética pois o mesmo alem de ser Grosso ficava olhando para as minhas pernas. Fiquei muito constrangida com a situação.\n",
    "Almoçamos tomamos. Sucos e cervejas, mesmo porque estavamos em 6 pessoas.\n",
    "No final pedimos a conta e o mesmo  trouxe duas contas uma com mais itens incluindo a refeiçao, outra somente com as bebidas,achei estranho porque tinham me cobrado normal das últimas vezes. Em fim.\n",
    "Como um gestor de um restaurante contrata pessoas sem conhecer seu carater?\n",
    "Tudo bem que a mão de obra de estrangeiros seja mais barata. Mas vocês teriam que pensar no bem estar dos clientes.\n",
    "Descobri o nome do garçom.\n",
    "CARLOS AMED. Ele é cubano.\n",
    "Ia fazer uma denuncia por assedio mas pensei na empresa. Por favor selecione melhor seus colaboradores.\n",
    "Muito indignada.'''\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Fui no dia 10/12/21 às 11:00 o restaurante não tinha ninguém ainda, fui fazer um pedido para viagem. Depois de quase 30 minutos de espera percebo que tinham clientes que chegaram depois comendo...questionei um garçom pela demora do pedido foi averiguar e não deu nenhuma satisfação. Aí percebi depois que o pedido estava no balcão aguardando o favor de algum garçom pegar e olha que nem estava lotado o restaurante...desrespeito com o cliente e garçons mal treinados.. nunca mais nesse lugar.\n",
      "<assistant>: Caro cliente, lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pela demora no atendimento e pela falta de atenção dos nossos garçons. Reconhecemos que isso causou uma má impressão e que não atendeu às suas expectativas. Levaremos suas observações em consideração para melhorar nossos serviços. Agradecemos por compartilhar sua experiência conosco e esperamos ter a oportunidade de recebê-lo novamente no futuro.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Fui no dia 10/12/21 às 11:00 o restaurante não tinha ninguém ainda, fui fazer um pedido para viagem. Depois de quase 30 minutos de espera percebo que tinham clientes que chegaram depois comendo...questionei um garçom pela demora do pedido foi averiguar e não deu nenhuma satisfação. Aí percebi depois que o pedido estava no balcão aguardando o favor de algum garçom pegar e olha que nem estava lotado o restaurante...desrespeito com o cliente e garçons mal treinados.. nunca mais nesse lugar.\"\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Só não dou zero porque não dá e porque a torta de frango estava boazinha. Priorizam quem tem aparência de ter mais dinheiro e os garçons não fazem questão alguma de atender as pessoas. Olharam na minha cara, passaram direto até que eu implorasse pra ser atendida. :) Não anotaram meu pedido e ainda queriam colocar a culpa em mim pelo esquecimento. Enfim, não volto!\n",
      "<assistant>: Caro cliente, lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo atendimento insatisfatório que você recebeu e pela falta de atenção dos nossos garçons. Levaremos suas críticas em consideração para melhorar nossos serviços. Quanto à torta de frango, vamos revisar nossos processos para garantir que todos os pratos sejam preparados de forma adequada. Agradecemos seu feedback e esperamos ter a oportunidade de recebê-lo novamente para uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Só não dou zero porque não dá e porque a torta de frango estava boazinha. Priorizam quem tem aparência de ter mais dinheiro e os garçons não fazem questão alguma de atender as pessoas. Olharam na minha cara, passaram direto até que eu implorasse pra ser atendida. :) Não anotaram meu pedido e ainda queriam colocar a culpa em mim pelo esquecimento. Enfim, não volto!\"\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Pedimos um prato de bife ao molho madeira sendo que o feijão veio com gosto de molho madeira e o bife veio com gosto de queijo então eu recomendaria que vocês se dedicassem mais e as atende vistas são muito de humor baixo\n",
      "<assistant>: Olá, agradecemos pelo seu feedback. Lamentamos muito pela experiência negativa que você teve com o nosso prato de bife ao molho madeira. Vamos investigar o ocorrido e tomar as medidas necessárias para garantir que isso não se repita. Quanto ao feijão, vamos revisar nossos processos de preparo para garantir que não haja confusão de sabores. Quanto às atitudes de nossos atendentes, vamos reforçar a importância do bom humor e da cordialidade. Agradecemos por nos informar sobre esses pontos negativos e esperamos ter a oportunidade de recebê-la novamente para uma experiência melhor.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Pedimos um prato de bife ao molho madeira sendo que o feijão veio com gosto de molho madeira e o bife veio com gosto de queijo então eu recomendaria que vocês se dedicassem mais e as atende vistas são muito de humor baixo\"\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Pedi no ifood camarão empanado e mandaram camarão alho e óleo muito sem graça e poucas unidades. Fui inventar de experimentar por causa da propaganda no Instagram e me ferrei! Não recomendo.\n",
      "<assistant>: Olá, agradecemos pelo seu feedback sobre a sua experiência com o nosso camarão empanado. Lamentamos que a qualidade do seu pedido não tenha atendido às suas expectativas. Vamos revisar nossos processos para garantir que isso não aconteça novamente. Agradecemos por nos informar e esperamos ter a oportunidade de melhor atendê-la no futuro.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Pedi no ifood camarão empanado e mandaram camarão alho e óleo muito sem graça e poucas unidades. Fui inventar de experimentar por causa da propaganda no Instagram e me ferrei! Não recomendo.\"\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1448: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/matheusalb/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "A comida é boa, mas o atendimento é bem ruim e ainda fazem de tudo pra fazer com que o cliente esteja errado. Esperei 1:10h pelo meu pedido (picanha pra 3p) e depois de ver 2 mesas de pessoas que chegaram depois de mim, receberem seus pedido, reclamei com o garçom, que disse q ainda tinham 4 pedidos na minha frente. Quase enlouqueci. Mas na verdade, eles entregaram meu pedido na outra mesa e eles não queriam admitir. Só não fui embora pq estávamos em grupo e todos com fome. E ainda demorou mais 15min depois disso. Pedi desconto no jantar e não deram nem desculpas pelo ocorrido. NÃO recomendo o local por isso.\n",
      "<assistant>: Caro cliente, lamentamos profundamente pela experiência negativa que você teve em nosso restaurante. Pedimos desculpas pelo atraso no seu pedido e pela falta de comunicação adequada sobre o ocorrido. Reconhecemos que isso causou uma má impressão e que não atendemos às suas expectativas em relação ao atendimento. Agradecemos por nos informar sobre esses pontos negativos e vamos trabalhar para melhorar esses aspectos. Quanto ao desconto solicitado, entendemos sua frustração e vamos revisar nossos procedimentos internos para evitar que isso aconteça novamente. Agradecemos por nos informar sobre essa questão e esperamos ter a oportunidade de recebê-lo novamente em nosso estabelecimento.\n"
     ]
    }
   ],
   "source": [
    "comment = \"A comida é boa, mas o atendimento é bem ruim e ainda fazem de tudo pra fazer com que o cliente esteja errado. Esperei 1:10h pelo meu pedido (picanha pra 3p) e depois de ver 2 mesas de pessoas que chegaram depois de mim, receberem seus pedido, reclamei com o garçom, que disse q ainda tinham 4 pedidos na minha frente. Quase enlouqueci. Mas na verdade, eles entregaram meu pedido na outra mesa e eles não queriam admitir. Só não fui embora pq estávamos em grupo e todos com fome. E ainda demorou mais 15min depois disso. Pedi desconto no jantar e não deram nem desculpas pelo ocorrido. NÃO recomendo o local por isso.\"\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Escreva, em Português, um comentário de resposta a ao seguinte comentário de um cliente ao seu restaurante: \n",
      "Salgados bem caros, sem tamanho ou sabor correspondente. Coxinhas por R$18 ou empanado por R$20. Compramos uma torta de R$200 e não nos permitiram comer a mesma na doceria? Tinha que pegar e ir comer em outro lugar pois é “política da empresa”. Lamentável, eu e minha família não retornamos mais, e somos consumidores fiéis de tortas por aí.\n",
      "<assistant>: Caro cliente, agradecemos por compartilhar sua experiência conosco. Lamentamos que a quantidade e o tamanho dos salgados não tenham atendido às suas expectativas. Vamos revisar nossos processos para garantir que isso não aconteça novamente. Quanto à política de não comer a torta em nosso estabelecimento, entendemos sua frustração e vamos reforçar a importância de respeitar essa regra. Agradecemos seu feedback e esperamos ter a oportunidade de recebê-lo novamente em nosso restaurante.\n"
     ]
    }
   ],
   "source": [
    "comment = \"Salgados bem caros, sem tamanho ou sabor correspondente. Coxinhas por R$18 ou empanado por R$20. Compramos uma torta de R$200 e não nos permitiram comer a mesma na doceria? Tinha que pegar e ir comer em outro lugar pois é “política da empresa”. Lamentável, eu e minha família não retornamos mais, e somos consumidores fiéis de tortas por aí.\"\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment='''Lugar supostamente temático, com artistas de acordo com o tema piratas, porém, tocando axe, pagode e tudo que não coincide com o local.\n",
    "\n",
    "Infelizmente é notório que o atendimento é enrolado, de baixa qualidade e muito demorado.\n",
    "\n",
    "São cobrada taxas pelos usos de brinquedos e entrada com bolo de aniversário. Além disso tivemos problemas para unir mesas, como se fosse um crime fazer isso.\n",
    "\n",
    "Não recomendo para pessoas que gostam de bom atendimento.'''\n",
    "\n",
    "inference(model, gen_input(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
